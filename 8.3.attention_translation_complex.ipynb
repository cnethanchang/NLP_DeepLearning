{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "'''\n",
    "见下面的jpg文件 \n",
    "本项目与attention教程中的第一个图类似\n",
    "b=BATCH_SIZE = 64\n",
    "d=EMBEDDING_SIZE = 300\n",
    "h=HIDDEN_SIZE = 512\n",
    "m_x=max_length_of_x  不同的batch中m_x不同\n",
    "m_y=max_length_of_y  不同的batch中m_x不同\n",
    "n=n_layers=3\n",
    "\n",
    "\n",
    "\n",
    "目标：翻译语句 I see. ==>Je comprends.\n",
    "原始数据，一行eg:    I see.\t   Je comprends.\n",
    "选择句子短语长度在3-25之间的翻译前后的句子,然后转化为数组  eg====> ['i', 'see', '.']           ['je', 'comprends', '.']\n",
    "每一句加上结束符号eg=====> ['i', 'see', '.' ,'</s>']               ['je', 'comprends', '.'  ,'</s>']\n",
    "转化为index   eg====>(tensor([[ 1614, 2251, 257, 3]]),       tensor([[ 1462,5955,257, 3]]))\n",
    "getBatch后        input补齐长度为max_x得到 (b,m_x,v_x)======(b,m_x,d)  \n",
    "                  target补齐长度为max_y得到 (b,m_y,v_y)\n",
    " \n",
    " \n",
    " \n",
    "encode     GRU( input,随机hidden)====>产生最后的hidden层   和enc_output  (d,m_x,2h)                2h是因为双向\n",
    "decode迭代m_y次， for i in range(m_y):    \n",
    "    第一次迭代dec_hidden随机初始化，dec_input即context来自于encode的最后hidden层\n",
    "    后来每一次迭代dec_hidden是上一次迭代产生的新dec_hidden (b,1,2h), \n",
    "                dec_input=context来自于      \n",
    "                attention=encode_output和新dec_hidden进行bnn操作= (b,m_x,2h)  bmm* (b,2h,1) = (b,m_x,1)         #相当于m_x层相似度检查，求内积，然后softmax\n",
    "                attention =  softmax (attention)      #沿着m_x方向\n",
    "                context = encode_output和attention进行bnn操作 = (b,m_x,2h)  bmm* (b,m_x,1)  = (b,2h,1)          #相当于m_x层attentin和enc_output乘机和\n",
    "\n",
    "                dec_output是         cat(新dec_hidden,context) = cat( (b,2h,1),(b,2h,1) ) = (b,4h,1)     \n",
    "                                    score = (v_y,4h) * (b,4h) = (v_y,b)  \n",
    "                                     softmaxed = F.log_softmax(score,1)        #沿着v_y方向softmax操作 \n",
    "                                 #softmaxed:(b,v_y)即为每一次迭代的输出，m_y次迭代即为(m_y,b,v_y) ,然后(m_y,b,v_y)和target做交叉熵得到loss\n",
    "\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter, OrderedDict\n",
    "import nltk\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "from torch.nn.utils.rnn import PackedSequence,pack_padded_sequence\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "random.seed(1024)\n",
    "%matplotlib inline\n",
    "\n",
    "FloatTensor = torch.FloatTensor\n",
    "LongTensor = torch.LongTensor\n",
    "ByteTensor = torch.ByteTensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBatch(batch_size, train_data):\n",
    "    random.shuffle(train_data)\n",
    "    sindex=0\n",
    "    eindex=batch_size\n",
    "    while eindex < len(train_data):\n",
    "        batch = train_data[sindex: eindex]\n",
    "        temp = eindex\n",
    "        eindex = eindex + batch_size\n",
    "        sindex = temp\n",
    "        yield batch\n",
    "    \n",
    "    if eindex >= len(train_data):\n",
    "        batch = train_data[sindex:]\n",
    "        yield batch\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "# It is for Sequence 2 Sequence format  #对于sentence.lenth<maxlenth的句子自动补齐长度\n",
    "def pad_to_batch(batch, x_to_ix, y_to_ix):\n",
    "    #按照source的句长从小到大排序\n",
    "    sorted_batch =  sorted(batch, key=lambda b:b[0].size(1), reverse=True) # sort by len\n",
    "    x,y = list(zip(*sorted_batch))  #x = [[(2,34,424,12),(34,24)......]]\n",
    "    max_x = max([s.size(1) for s in x])\n",
    "    max_y = max([s.size(1) for s in y])  #s.size(1)理解为sentence的长度 ，s.size(1)=1 或者V\n",
    "    x_p, y_p = [], []\n",
    "    for i in range(len(batch)):\n",
    "        if x[i].size(1) < max_x:          #对于sentence.lenth<maxlenth的句子自动补齐长度\n",
    "            x_p.append(   torch.cat(   [x[i],    Variable(LongTensor([x_to_ix['<PAD>']] * (max_x - x[i].size(1))   )).view(1, -1)], 1  )   )\n",
    "        else:\n",
    "            x_p.append(x[i])\n",
    "        if y[i].size(1) < max_y:\n",
    "            y_p.append(  torch.cat([y[i], Variable(LongTensor([y_to_ix['<PAD>']] * (max_y - y[i].size(1)))).view(1, -1)], 1))\n",
    "        else:\n",
    "            y_p.append(y[i])\n",
    "        \n",
    "    input_var = torch.cat(x_p)  #input_var  = [[(2,34,424,12),(34,24,0,0)......]]\n",
    "    target_var = torch.cat(y_p)\n",
    "    #input_len表示source   input句子中原有句子的长度\n",
    "    input_len = [   list(map(lambda s: s ==0, t.data)).count(False)    for t in input_var  ]  #input_len=[4,2....]     (b)\n",
    "    target_len = [list(map(lambda s: s ==0, t.data)).count(False) for t in target_var]\n",
    "    \n",
    "    return input_var, target_var, input_len, target_len\n",
    "\n",
    "\n",
    "\n",
    "def prepare_sequence(seq, to_index):\n",
    "    idxs = list(map(lambda w: to_index[w] if to_index.get(w) is not None else to_index[\"<UNK>\"], seq))\n",
    "    return Variable(LongTensor(idxs))\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalize_string(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([,.!?])\", r\" \\1 \", s)\n",
    "    s = re.sub(r\"[^a-zA-Z,.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170190\n30000\nGo.\tVa !\nHi.\tSalut !\nHi.\tSalut.\nRun!\tCours !\nRun!\tCourez !\nWho?\tQui ?\nWow!\tÇa alors !\nFire!\tAu feu !\nHelp!\tÀ l'aide !\nJump.\tSaute.\nStop!\tÇa suffit !\nStop!\tStop !\nStop!\tArrête-toi !\nWait!\tAttends !\nWait!\tAttendez !\nGo on.\tPoursuis.\nGo on.\tContinuez.\nGo on.\tPoursuivez.\nHello!\tBonjour !\nHello!\tSalut !\nI see.\tJe comprends.\nI try.\tJ'essaye.\n"
     ]
    }
   ],
   "source": [
    "corpus = open(r'C:\\workspace\\python-work\\nlp\\NLP-cs224n\\NLP-cs224n_ec\\notebooks\\dataset\\translation\\eng-fra.txt','r',encoding='utf-8').readlines()\n",
    "print(len(corpus))\n",
    "corpus = corpus[:30000]  # for practice\n",
    "print(len(corpus))\n",
    "for i in range(22):\n",
    "    print(corpus[i].strip())\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I see. ===> ['i', 'see', '.'] \n Je comprends. ====> ['je', 'comprends', '.']\nI try. ===> ['i', 'try', '.'] \n J'essaye. ====> ['j', 'essaye', '.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'see', '.'], ['i', 'try', '.'], ['i', 'won', '!'], ['i', 'won', '!'], ['i', 'won', '.'], ['oh', 'no', '!'], ['get', 'up', '.'], ['go', 'now', '.'], ['go', 'now', '.'], ['go', 'now', '.']]\n[['je', 'comprends', '.'], ['j', 'essaye', '.'], ['j', 'ai', 'gagne', '!'], ['je', 'l', 'ai', 'emporte', '!'], ['j', 'ai', 'gagne', '.'], ['oh', 'non', '!'], ['leve', 'toi', '.'], ['va', ',', 'maintenant', '.'], ['allez', 'y', 'maintenant', '.'], ['vas', 'y', 'maintenant', '.']]\n29794 29794\n['i', 'see', '.'] ['je', 'comprends', '.']\nWall time: 1.15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "X_r,y_r=[],[]\n",
    "     \n",
    "MIN_LENGTH = 3\n",
    "MAX_LENGTH = 25\n",
    "tmp=0\n",
    "\n",
    "#选择句子短语长度在3——25之间的翻译前后的句子,然后转化为数组    #   \"I see.\" ===>   ['i', 'see', '.'] \n",
    "for parallel in corpus: \n",
    "    tmp+=1\n",
    "    so,ta = parallel[:-1].split('\\t')\n",
    "    if so.strip() == \"\" or ta.strip() == \"\": \n",
    "        continue\n",
    "    \n",
    "    normalized_so = normalize_string(so).split()       #   \"I see.\" ===>   ['i', 'see', '.'] \n",
    "    normalized_ta = normalize_string(ta).split()       #    \"Je comprends. \"  =====>  ['je', 'comprends', '.']\n",
    "    \n",
    "    \n",
    "    if (tmp==21):\n",
    "        print(so,'===>',normalized_so,'\\n',ta,'====>',normalized_ta)\n",
    "    if (tmp==22):\n",
    "        print(so,'===>',normalized_so,'\\n',ta,'====>',normalized_ta)\n",
    "        \n",
    "    if len(normalized_so) >= MIN_LENGTH and len(normalized_so) <= MAX_LENGTH and len(normalized_ta) >= MIN_LENGTH and len(normalized_ta) <= MAX_LENGTH:\n",
    "        X_r.append(normalized_so)\n",
    "        y_r.append(normalized_ta)\n",
    "    \n",
    "print(X_r[:10])\n",
    "print(y_r[:10])\n",
    "\n",
    "print(len(X_r), len(y_r))\n",
    "print(X_r[0], y_r[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4401 7589\n"
     ]
    }
   ],
   "source": [
    "source_vocab = list(set(flatten(X_r)))\n",
    "target_vocab = list(set(flatten(y_r)))\n",
    "print(len(source_vocab), len(target_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "source2index = {'<PAD>': 0, '<UNK>': 1, '<s>': 2, '</s>': 3}\n",
    "for vo in source_vocab:\n",
    "    if source2index.get(vo) is None:\n",
    "        source2index[vo] = len(source2index)\n",
    "index2source = {v:k for k, v in source2index.items()}\n",
    "\n",
    "\n",
    "\n",
    "target2index = {'<PAD>': 0, '<UNK>': 1, '<s>': 2, '</s>': 3}\n",
    "for vo in target_vocab:\n",
    "    if target2index.get(vo) is None:\n",
    "        target2index[vo] = len(target2index)\n",
    "index2target = {v:k for k, v in target2index.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 4076,  3067,  1074,     3]]), tensor([[ 6171,  1531,  4166,     3]]))\n(tensor([[ 4076,  2437,  1074,     3]]), tensor([[ 3087,  5430,  4166,     3]]))\n(tensor([[ 4076,  2007,   416,     3]]), tensor([[ 3087,  2522,  5098,  3887,     3]]))\n(tensor([[ 4076,  2007,   416,     3]]), tensor([[ 6171,  2338,  2522,  3513,  3887,     3]]))\n(tensor([[ 4076,  2007,  1074,     3]]), tensor([[ 3087,  2522,  5098,  4166,     3]]))\n(tensor([[ 4009,  2840,   416,     3]]), tensor([[ 1694,  6879,  3887,     3]]))\n(tensor([[  584,  1632,  1074,     3]]), tensor([[ 3685,  5446,  4166,     3]]))\n(tensor([[  902,  1342,  1074,     3]]), tensor([[ 4831,  3621,  4256,  4166,     3]]))\n(tensor([[  902,  1342,  1074,     3]]), tensor([[ 7277,  4720,  4256,  4166,     3]]))\n(tensor([[  902,  1342,  1074,     3]]), tensor([[ 6614,  4720,  4256,  4166,     3]]))\nWall time: 591 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n(tensor([[ 1614, 2251, 257, 3]]), tensor([[ 1462,5955,5735,3]]))         \\n上面解释  word2index  ['i', 'see', '.',  '</s>'] ======>[[ 1614, 2251, 257, 3]]       ['je', 'comprends', '.','</s>']=====>[[ 1462,  5955, 5735, 3]]\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "X_p, y_p = [], []\n",
    "\n",
    "for so, ta in zip(X_r, y_r):\n",
    "    X_p.append(prepare_sequence(so + ['</s>'], source2index).view(1, -1))\n",
    "    y_p.append(prepare_sequence(ta + ['</s>'], target2index).view(1, -1))\n",
    "    \n",
    "train_data = list(zip(X_p, y_p))\n",
    "\n",
    "for i in range(10):\n",
    "    print(train_data[i])\n",
    "    \n",
    "    \n",
    "'''\n",
    "(tensor([[ 1614, 2251, 257, 3]]), tensor([[ 1462,5955,5735,3]]))         \n",
    "上面解释  word2index  ['i', 'see', '.',  '</s>'] ======>[[ 1614, 2251, 257, 3]]       ['je', 'comprends', '.','</s>']=====>[[ 1462,  5955, 5735, 3]]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size,hidden_size, n_layers=1,bidirec=False):      #n=nlayers=3\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, embedding_size) #(v,d)\n",
    "        \n",
    "        if bidirec:\n",
    "            self.n_direction = 2 \n",
    "            self.gru = nn.GRU(embedding_size, hidden_size, n_layers, batch_first=True, bidirectional=True)  \n",
    "        else:\n",
    "            self.n_direction = 1\n",
    "            self.gru = nn.GRU(embedding_size, hidden_size, n_layers, batch_first=True)\n",
    "    \n",
    "    def init_hidden(self, inputs):\n",
    "        #inputs.size(0)  =  B    (6,B,H)\n",
    "        hidden = Variable(torch.zeros(self.n_layers * self.n_direction, inputs.size(0), self.hidden_size))      #(2*n,b,h)\n",
    "        return hidden\n",
    "    \n",
    "    def init_weight(self):\n",
    "        self.embedding.weight = nn.init.xavier_uniform(self.embedding.weight)\n",
    "        self.gru.weight_hh_l0 = nn.init.xavier_uniform(self.gru.weight_hh_l0)\n",
    "        self.gru.weight_ih_l0 = nn.init.xavier_uniform(self.gru.weight_ih_l0)\n",
    "    \n",
    "    def forward(self, inputs, input_lengths):         # (b,m_x,v), (b) \n",
    "        \"\"\"\n",
    "        input_lengths : real lengths of input batch (list)\n",
    "        \"\"\"\n",
    "        hidden = self.init_hidden(inputs)  #(2*n,b,h)=(6,b,h)\n",
    "        \n",
    "        \n",
    "        embedded = self.embedding(inputs)  # (d,v) * (b,m_x,v)=(b,m_x,d)\n",
    "        \n",
    "        packed = pack_padded_sequence(embedded, input_lengths, batch_first=True)\n",
    "        #outputs:(m_x,2*h,b)    hidden:(6,h,b)\n",
    "        outputs, hidden = self.gru(packed, hidden)       #gru( (b,m_x,d),(6,b,h)  )\n",
    "      \n",
    "        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True) # unpack (back to padded)\n",
    "                \n",
    "        if self.n_layers > 1:\n",
    "            if self.n_direction == 2:\n",
    "                hidden = hidden[-2:]\n",
    "            else:\n",
    "                hidden = hidden[-1]\n",
    "        \n",
    "        return outputs, torch.cat([h for h in hidden], 1).unsqueeze(1)      #outputs:(m_x,2*h,b)    hidden:(2h,b) \n",
    "    \n",
    "   \n",
    "   \n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, n_layers=1, dropout_p=0.1): # Decoder(v_y, d,2h)\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # Define the layers\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)  #(v_y,d)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        self.gru = nn.GRU(embedding_size + hidden_size, hidden_size, n_layers, batch_first=True)   #(d+2h,2h_\n",
    "        self.linear = nn.Linear(hidden_size * 2, input_size) #(4h,v_y)\n",
    "        self.attn = nn.Linear(self.hidden_size, self.hidden_size) # Attention  #(2h,2h)\n",
    "    \n",
    "    def init_hidden(self,inputs):\n",
    "        hidden = Variable(torch.zeros(self.n_layers, inputs.size(0), self.hidden_size))  #(n,v_y,2h)=(1,v_y,2h)\n",
    "        return  hidden\n",
    "    \n",
    "    \n",
    "    def init_weight(self):\n",
    "        self.embedding.weight = nn.init.xavier_uniform(self.embedding.weight)\n",
    "        self.gru.weight_hh_l0 = nn.init.xavier_uniform(self.gru.weight_hh_l0)\n",
    "        self.gru.weight_ih_l0 = nn.init.xavier_uniform(self.gru.weight_ih_l0)\n",
    "        self.linear.weight = nn.init.xavier_uniform(self.linear.weight)\n",
    "        self.attn.weight = nn.init.xavier_uniform(self.attn.weight)\n",
    "#         self.attn.bias.data.fill_(0)\n",
    "    \n",
    "    def Attention(self, hidden, encoder_outputs, encoder_maskings):      #self.Attentin( (1,b,2h),(b,m_x,2h) ,none  )\n",
    "        \"\"\"\n",
    "        hidden : (1,b,2h)\n",
    "        encoder_outputs :(b,m_x,2h)\n",
    "        encoder_maskings : B,T # ByteTensor\n",
    "        \"\"\"\n",
    "        hidden = hidden[0].unsqueeze(2)  # (1,b,2h) ->  (b,2h) ---->(b,2h,1)\n",
    "        batch_size = encoder_outputs.size(0) # b\n",
    "        max_len = encoder_outputs.size(1) # m_x\n",
    "        energies = self.attn(encoder_outputs.contiguous().view(batch_size * max_len, -1)) # (b,m_x,2h) ===>(b*m_x,2h)    (2h,2h)*(b*m_x,2h)=(b*m_x,2h)\n",
    "        energies = energies.view(batch_size,max_len, -1) #(b*m_x,2h)===>(b,m_x,2h)\n",
    "        \n",
    "        #求attention  内积操作  得到attention:(b,m_x)    然后softmax\n",
    "        attn_energies = energies.bmm(hidden).squeeze(2) # B,T,D * B,D,1 --> B,T           (b,m_x,2h) bmm  (b,2h,1) =====>(b,m_x,1)=====>(b,m_x)\n",
    "        alpha = F.softmax(attn_energies,1) # (b,m_x)\n",
    "        alpha = alpha.unsqueeze(1) # (b,1，m_x)\n",
    "        \n",
    "        \n",
    "        #求context    endoder_output 和 attention 运行bmm（batch  matrix multip)操作得到context= (b,1,2h) \n",
    "#                                                      <=============>  等效于每一层output和其attention的乘积，然后相加\n",
    "        context = alpha.bmm(encoder_outputs)  #(b,1，m_x)  * (b,m_x,2h) ---->(b,1,2h) \n",
    "        \n",
    "        return context, alpha               #alpha相当于softmax之后的attention\n",
    "    \n",
    "    # forward(  (b,1),(b,1,2h),max_y,(m_x,2*h,b) , (b,max_x)  )\n",
    "    def forward(self, inputs, context, max_length, encoder_outputs, encoder_maskings=None, is_training=False):\n",
    "        \"\"\"\n",
    "        inputs : B,1 (LongTensor, START SYMBOL)\n",
    "        context : B,1,2h (FloatTensor, Last encoder hidden state)\n",
    "        max_length : int, max length to decode # for batch\n",
    "        encoder_outputs : b,max_x,2h\n",
    "        encoder_maskings : B,max_x # ByteTensor\n",
    "        is_training : bool, this is because adapt dropout only training step.\n",
    "        \"\"\"\n",
    "        # Get the embedding of the current input word\n",
    "        embedded = self.embedding(inputs)      # (v_y,d)*(b,1)====>(v_y,d)*(b,v_y)=(b,1,d)\n",
    "        \n",
    "        hidden = self.init_hidden(inputs)       #(1,v_y,2h)*(b,1)=======>(1,v_y,2h)*(b,v_y)=(1,b,2h)\n",
    "        \n",
    "        if is_training:\n",
    "            embedded = self.dropout(embedded)\n",
    "        \n",
    "        decode = []\n",
    "        # Apply GRU to the output so far\n",
    "        for i in range(max_length): #m_y\n",
    "            # tmp=torch.cat((embedded, context),2)    #(b,1,d)  cat  (b,1,2h) ===>(b,1,d+2h)\n",
    "            \n",
    "            #hidden:(1,b,2h)\n",
    "            _, hidden = self.gru(torch.cat((embedded, context), 2), hidden)     #self.gru(  (b,1,d+2h) ,(1,b,2h) )\n",
    "            \n",
    "            concated = torch.cat((hidden, context.transpose(0, 1)), 2) # (1,b,4h)\n",
    "            score = self.linear(concated.squeeze(0))                   # (1,b,4h)*(4h,v_y)= (b,v_y)\n",
    "            softmaxed = F.log_softmax(score,1)        #沿着v_y方向softmax操作      (b,v_y)\n",
    "        \n",
    "            #这是用于计算lossfunction的预测数据， softmaxed:(b,v_y)\n",
    "            decode.append(softmaxed)\n",
    "            \n",
    "            decoded = softmaxed.max(1)[1]       #decoded:(64)=(b)\n",
    "            embedded = self.embedding(decoded).unsqueeze(1) #(d,v_y) * (b) ==>(b,1,d)\n",
    "            if is_training:\n",
    "                embedded = self.dropout(embedded)\n",
    "            \n",
    "            # compute next context vector using attention   \n",
    "            #contex：(b,1,2h)           #alpha相当于softmax之后的attention ：(b,1，m_x)\n",
    "            context, alpha = self.Attention(hidden, encoder_outputs, encoder_maskings)     #self.Attentin( (1,b,2h),(m_x,2*h,b) ,none  )\n",
    "            \n",
    "        #  column-wise concat, reshape!!\n",
    "        #decode是数组，长度为max_y,每个元素是softmaxed :(b,v_y)          max_length=max_y\n",
    "        scores = torch.cat(decode, 1)\n",
    "    \n",
    "        return scores.view(inputs.size(0) * max_length, -1)    #(b*max_y,v_y)\n",
    "\n",
    "    \n",
    "    #测试的时候用\n",
    "    #context为decode最后hidden:(2h,b)    #encoder_outputs:(m_x,2*h,b)           2h是因为双向 (b,2,h)===>(b,2h)\n",
    "    def decode(self, context, encoder_outputs):\n",
    "        start_decode = Variable(LongTensor([[target2index['<s>']] * 1])).transpose(0, 1) #(b,1)  此处b=1\n",
    "        embedded = self.embedding(start_decode)    #   (v_y,d)*(b,1)====>(v_y,d)*(b,v_y)=(b,1,d)\n",
    "        hidden = self.init_hidden(start_decode)     #(1,v_y,2h)*(b,1)=======>(1,v_y,2h)*(b,v_y)=(1,b,2h)\n",
    "        \n",
    "        decodes = []\n",
    "        attentions = []\n",
    "        decoded = embedded\n",
    "        while decoded.data.tolist()[0] != target2index['</s>']: # until </s>    相当于 for i in range(max_y):,知道预测到结束符\n",
    "             # tmp=torch.cat((embedded, context),2)    #(b,1,d)  cat  (b,1,2h) ===>(b,1,d+2h)\n",
    "            #hidden:(1,b,2h)\n",
    "            _, hidden = self.gru(torch.cat((embedded, context), 2), hidden)   #self.gru(  (b,1,d+2h) ,(1,b,2h) )\n",
    "             \n",
    "            concated = torch.cat((hidden, context.transpose(0, 1)), 2)  # (1,b,4h)\n",
    "            score = self.linear(concated.squeeze(0))               # (1,b,4h)*(4h,v_y)= (b,v_y)\n",
    "             \n",
    "            \n",
    "            softmaxed = F.log_softmax(score,1)                 #沿着v_y方向softmax操作      (b,v_y)\n",
    "            #decodes这是用于记录预测结果， softmaxed:(b,v_y)\n",
    "            decodes.append(softmaxed)\n",
    "              \n",
    "            decoded = softmaxed.max(1)[1]      #decoded:(64)=(b)\n",
    "            embedded = self.embedding(decoded).unsqueeze(1) #(d,v_y) * (b) ==>(b,1,d)\n",
    "             \n",
    "             # compute next context vector using attention   \n",
    "            #contex：(b,1,2h)           #alpha相当于softmax之后的attention ：(b,1，m_x)\n",
    "            context, alpha = self.Attention(hidden, encoder_outputs,None)        #self.Attentin( (1,b,2h),(m_x,2*h,b) ,none  )\n",
    "            #alpha相当于softmax之后的attention ：(b,1，m_x)\n",
    "            attentions.append(alpha.squeeze(1))\n",
    "        \n",
    "        return torch.cat(decodes).max(1)[1], torch.cat(attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\software_installed_cs\\Anaconda3\\envs\\pytorch0.4\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\nC:\\software_installed_cs\\Anaconda3\\envs\\pytorch0.4\\lib\\site-packages\\ipykernel_launcher.py:25: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\nC:\\software_installed_cs\\Anaconda3\\envs\\pytorch0.4\\lib\\site-packages\\ipykernel_launcher.py:26: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\nC:\\software_installed_cs\\Anaconda3\\envs\\pytorch0.4\\lib\\site-packages\\ipykernel_launcher.py:76: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\nC:\\software_installed_cs\\Anaconda3\\envs\\pytorch0.4\\lib\\site-packages\\ipykernel_launcher.py:77: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\nC:\\software_installed_cs\\Anaconda3\\envs\\pytorch0.4\\lib\\site-packages\\ipykernel_launcher.py:78: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\nC:\\software_installed_cs\\Anaconda3\\envs\\pytorch0.4\\lib\\site-packages\\ipykernel_launcher.py:79: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\software_installed_cs\\Anaconda3\\envs\\pytorch0.4\\lib\\site-packages\\ipykernel_launcher.py:80: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\software_installed_cs\\Anaconda3\\envs\\pytorch0.4\\lib\\site-packages\\ipykernel_launcher.py:46: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\nC:\\software_installed_cs\\Anaconda3\\envs\\pytorch0.4\\lib\\site-packages\\ipykernel_launcher.py:47: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/2] [000/465] mean_loss : 8.93\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/2] [010/465] mean_loss : 6.46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/2] [020/465] mean_loss : 5.18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/2] [030/465] mean_loss : 4.93\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/2] [040/465] mean_loss : 4.78\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/2] [050/465] mean_loss : 4.50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/2] [060/465] mean_loss : 4.44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/2] [070/465] mean_loss : 4.34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/2] [080/465] mean_loss : 4.34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/2] [090/465] mean_loss : 4.14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/2] [100/465] mean_loss : 4.06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/2] [110/465] mean_loss : 4.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/2] [120/465] mean_loss : 3.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/2] [130/465] mean_loss : 3.91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/2] [140/465] mean_loss : 3.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/2] [150/465] mean_loss : 3.88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/2] [160/465] mean_loss : 3.82\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/2] [170/465] mean_loss : 3.77\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/2] [180/465] mean_loss : 3.80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/2] [190/465] mean_loss : 3.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/2] [200/465] mean_loss : 3.80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/2] [210/465] mean_loss : 3.66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/2] [220/465] mean_loss : 3.70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/2] [230/465] mean_loss : 3.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/2] [240/465] mean_loss : 3.67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/2] [250/465] mean_loss : 3.60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/2] [260/465] mean_loss : 3.71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/2] [270/465] mean_loss : 3.63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/2] [280/465] mean_loss : 3.63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/2] [290/465] mean_loss : 3.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/2] [300/465] mean_loss : 3.54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/2] [310/465] mean_loss : 3.49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/2] [320/465] mean_loss : 3.43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/2] [330/465] mean_loss : 3.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/2] [340/465] mean_loss : 3.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/2] [350/465] mean_loss : 3.36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/2] [360/465] mean_loss : 3.43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/2] [370/465] mean_loss : 3.36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/2] [380/465] mean_loss : 3.29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/2] [390/465] mean_loss : 3.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/2] [400/465] mean_loss : 3.29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/2] [410/465] mean_loss : 3.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/2] [420/465] mean_loss : 3.27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/2] [430/465] mean_loss : 3.24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/2] [440/465] mean_loss : 3.15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/2] [450/465] mean_loss : 3.23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/2] [460/465] mean_loss : 3.20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/2] [000/465] mean_loss : 2.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/2] [010/465] mean_loss : 2.96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/2] [020/465] mean_loss : 2.88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/2] [030/465] mean_loss : 2.90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/2] [040/465] mean_loss : 2.92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/2] [050/465] mean_loss : 2.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/2] [060/465] mean_loss : 2.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/2] [070/465] mean_loss : 2.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/2] [080/465] mean_loss : 2.79\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/2] [090/465] mean_loss : 2.70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/2] [100/465] mean_loss : 2.73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/2] [110/465] mean_loss : 2.82\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/2] [120/465] mean_loss : 2.78\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/2] [130/465] mean_loss : 2.80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/2] [140/465] mean_loss : 2.74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/2] [150/465] mean_loss : 2.74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/2] [160/465] mean_loss : 2.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/2] [170/465] mean_loss : 2.61\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/2] [180/465] mean_loss : 2.76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/2] [190/465] mean_loss : 2.69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/2] [200/465] mean_loss : 2.70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/2] [210/465] mean_loss : 2.71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/2] [220/465] mean_loss : 2.64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/2] [230/465] mean_loss : 2.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/2] [240/465] mean_loss : 2.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/2] [250/465] mean_loss : 2.66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/2] [260/465] mean_loss : 2.60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/2] [270/465] mean_loss : 2.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/2] [280/465] mean_loss : 2.50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/2] [290/465] mean_loss : 2.49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/2] [300/465] mean_loss : 2.51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/2] [310/465] mean_loss : 2.54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/2] [320/465] mean_loss : 2.51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/2] [330/465] mean_loss : 2.43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/2] [340/465] mean_loss : 2.53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/2] [350/465] mean_loss : 2.45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/2] [360/465] mean_loss : 2.47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/2] [370/465] mean_loss : 2.59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/2] [380/465] mean_loss : 2.53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/2] [390/465] mean_loss : 2.51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/2] [400/465] mean_loss : 2.43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/2] [410/465] mean_loss : 2.47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/2] [420/465] mean_loss : 2.37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/2] [430/465] mean_loss : 2.44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/2] [440/465] mean_loss : 2.52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/2] [450/465] mean_loss : 2.47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/2] [460/465] mean_loss : 2.37\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 2\n",
    "BATCH_SIZE = 64\n",
    "EMBEDDING_SIZE = 300\n",
    "HIDDEN_SIZE = 512\n",
    "LR = 0.001\n",
    "DECODER_LEARNING_RATIO = 5.0\n",
    "RESCHEDULED = False\n",
    "\n",
    "encoder = Encoder(len(source2index), EMBEDDING_SIZE, HIDDEN_SIZE, 3, True) #bidirectional\n",
    "decoder = Decoder(len(target2index), EMBEDDING_SIZE, HIDDEN_SIZE * 2)\n",
    "encoder.init_weight()\n",
    "decoder.init_weight()\n",
    "\n",
    "\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
    "enc_optimizer = optim.Adam(encoder.parameters(), lr=LR)\n",
    "dec_optimizer = optim.Adam(decoder.parameters(), lr=LR * DECODER_LEARNING_RATIO)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    losses=[]\n",
    "    for i, batch in enumerate(getBatch(BATCH_SIZE, train_data)):\n",
    "        \n",
    "    #input_var  = [[(2,34,424,12),(34,24,0,0)......]]  长度不够的0自动补齐       #input_len表示source  input句子中原有句子的长度 eg: input_len=[4,2....]  (b))\n",
    "        inputs, targets, input_lengths, target_lengths = pad_to_batch(batch, source2index, target2index)\n",
    "        # inputs.size()=(b,max_x)=(64,8)            targets.size()=(b,max_y)=(64,15)    \n",
    "        #input_masks表示inputs数组中每一个元素是否是自动补齐的<P>   (b,max_x)=(64,8)  每一个元素是一个二元组（data，bool）\n",
    "        input_masks = torch.cat([Variable(ByteTensor(tuple(map(lambda s: s ==0, t.data)))) for t in inputs]).view(inputs.size(0), -1)\n",
    "        \n",
    "        #start_decode:(b,1)\n",
    "        start_decode = Variable(LongTensor([[target2index['<s>']] * targets.size(0)])).transpose(0, 1)   #targets.size(0)=b\n",
    "    \n",
    "        encoder.zero_grad()\n",
    "        decoder.zero_grad()\n",
    "         #output:(m_x,2*h,b)    hidden_c:(2h,b) \n",
    "        output, hidden_c = encoder(inputs, input_lengths)         #encoder(   (b,m_x,v),(b) )\n",
    "        \n",
    "        #preds:(b*max_y,v_y)  是softmax之后预测的值\n",
    "        preds = decoder(start_decode, hidden_c, targets.size(1), output, input_masks, True)#decoder( (b,1),(2h,b),max_y,(m_x,2*h,b) , (b,max_x)  )\n",
    "        #target:(b,max_y)             \n",
    "        loss = loss_function(preds, targets.view(-1))\n",
    "        losses.append(loss.data.tolist() )\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(encoder.parameters(), 50.0) # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm(decoder.parameters(), 50.0) # gradient clipping\n",
    "        enc_optimizer.step()\n",
    "        dec_optimizer.step()\n",
    "\n",
    "        if i % 10==0:\n",
    "            print(\"[%02d/%d] [%03d/%d] mean_loss : %0.2f\" %(epoch, EPOCH, i, len(train_data)//BATCH_SIZE, np.mean(losses)))\n",
    "            losses=[]\n",
    "\n",
    "    # You can use http://pytorch.org/docs/master/optim.html#how-to-adjust-learning-rate\n",
    "    if RESCHEDULED == False and epoch  == EPOCH//2:\n",
    "        LR *= 0.01\n",
    "        enc_optimizer = optim.Adam(encoder.parameters(), lr=LR)\n",
    "        dec_optimizer = optim.Adam(decoder.parameters(), lr=LR * DECODER_LEARNING_RATIO)\n",
    "        RESCHEDULED = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_attention(input_words, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_words, rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "#     show_plot_visdom()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source :  i m tired of boston .\nTruth :  je suis fatigue de boston .\nPrediction :  je suis fatiguee de boston . \n\n\n\ninput= ['i', 'm', 'tired', 'of', 'boston', '.', '</s>']\ntruth= tensor([[ 6171,    74,  1198,  2653,    50,  4166,     3]])\nprediction= ['je', 'suis', 'fatiguee', 'de', 'boston', '.', '</s>']\nattention= torch.Size([7, 7]) \n\n\n tensor([[ 9.9977e-01,  2.2760e-04,  4.4660e-22,  5.0131e-21,  7.3736e-23,\n          1.0741e-19,  3.7500e-23],\n        [ 3.0157e-07,  1.0000e+00,  1.4388e-07,  1.1932e-15,  4.9877e-23,\n          2.9466e-22,  5.1918e-23],\n        [ 4.1070e-18,  4.9535e-21,  1.1506e-10,  9.9972e-01,  2.7535e-04,\n          1.2978e-22,  4.0850e-13],\n        [ 7.3707e-26,  5.8070e-32,  2.4948e-20,  8.4258e-09,  1.0000e+00,\n          1.8944e-28,  4.9664e-25],\n        [ 1.1799e-33,  7.2128e-37,  2.9370e-20,  1.7539e-12,  1.0000e+00,\n          3.8148e-28,  5.9441e-28],\n        [ 1.5854e-32,  3.0696e-35,  6.0884e-22,  1.2827e-16,  1.0000e+00,\n          2.5257e-24,  7.3656e-27],\n        [ 2.1849e-33,  7.4314e-35,  9.7024e-22,  4.1154e-18,  1.0000e+00,\n          2.1488e-22,  8.3472e-25]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = random.choice(train_data)\n",
    "input_ = test[0]\n",
    "truth = test[1]\n",
    "\n",
    "output, hidden = encoder(input_, [input_.size(1)])\n",
    "pred, attn = decoder.decode(hidden, output)\n",
    "\n",
    "input_ = [index2source[i] for i in input_.data.tolist()[0]]\n",
    "pred = [index2target[i] for i in pred.data.tolist()]\n",
    "\n",
    "\n",
    "print('Source : ',' '.join([i for i in input_ if i not in ['</s>']]))\n",
    "print('Truth : ',' '.join([index2target[i] for i in truth.data.tolist()[0] if i not in [2, 3]]))\n",
    "print('Prediction : ',' '.join([i for i in pred if i not in ['</s>']]),\"\\n\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"input=\",input_)\n",
    "print(\"truth=\",truth)\n",
    "print(\"prediction=\",pred)\n",
    "print(\"attention=\",attn.data.size(),'\\n\\n\\n',attn.data)\n",
    "\n",
    "show_attention(input_, pred, attn.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
