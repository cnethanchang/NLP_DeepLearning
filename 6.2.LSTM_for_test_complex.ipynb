{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter, OrderedDict\n",
    "import nltk\n",
    "from copy import deepcopy\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "random.seed(1024)\n",
    "\n",
    "\n",
    "FloatTensor = torch.FloatTensor\n",
    "LongTensor = torch.LongTensor\n",
    "ByteTensor = torch.ByteTensor\n",
    "\n",
    "#本案例(见下图)中利用文章前面的词预测后面的词       seq_lenth = max_length\n",
    "#文章所有的词汇分为16（BATCH_SIZE）份 ，共有length个单词。  得到矩阵 （16，length/16)\n",
    "#getch方法将（16，length/16) 矩阵沿着length/16方向分为 一个个 (16,seq_lenth)的小矩阵\n",
    "#得到 x = （16，seq_length,v) = (b,max_length,v) ==>(m,b,v)\n",
    "#y为X矩阵沿着length/16方向下移一个单位的矩阵   (m,b,v)\n",
    "\n",
    "\n",
    "\n",
    "#      X :(m,b,v)\n",
    "#hidden_state =  [num_layers(=1) * num_directions(=1), b, h]=(1,b,h)\n",
    "# cell_state =   [num_layers(=1) * num_directions(=1), b, h]=(1,b,h)\n",
    "\n",
    "\n",
    "# self.lstm = nn.LSTM(input_size=v, hidden_size=h)   #(v,h)   \n",
    "#LSTM 操作：    outputs, (_, _) = self.lstm(input, (hidden_state, cell_state))             #self.lstm(   (m,b,v) , ( (1,b,h),(1,b,h)  )  \n",
    "\n",
    " # 得到的outputs : [m, b, num_directions(=1) * h] = (m,b,h)\n",
    "\n",
    "\n",
    "# 然后output乘以矩阵： (v,h) * (m,b,h) = (m,b,v)       \n",
    "#(m,b,v)与实际的y做softmax得到loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_index):\n",
    "    idxs = list(map(lambda w: to_index[w] if to_index.get(w) is not None else to_index[\"<unk>\"], seq))\n",
    "    return LongTensor(idxs)\n",
    "\n",
    "\n",
    "def prepare_ptb_dataset(filename, word2index=None):\n",
    "    corpus = open(filename, 'r', encoding='utf-8').readlines()\n",
    "    corpus = flatten([co.strip().split() + ['</s>'] for co in corpus])        #corpus  是 （1，len(word) ),不同行的sentence混合了\n",
    "    #每句话以</s>结尾\n",
    "    if word2index == None:\n",
    "        vocab = list(set(corpus))\n",
    "        word2index = {'<unk>': 0}\n",
    "        for vo in vocab:\n",
    "            if word2index.get(vo) is None:\n",
    "                word2index[vo] = len(word2index)\n",
    "    #返回corpus中word对应的index的数组\n",
    "    return prepare_sequence(corpus, word2index), word2index\n",
    "\n",
    "\n",
    "#将batch_size*nbatch多余的部分去掉\n",
    "def batchify(data, bsz):\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).   将batch_size*nbatch多余的部分去掉\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).contiguous()\n",
    "    # print(\"data.size()=\",data.size())     #  data.size()= torch.Size([16, 58099])\n",
    "    # print(\"data.size(1)=\",data.size(1))    #   data.size(1)= 58099\n",
    "    # print(\"data=\",data)\n",
    "  \n",
    " \n",
    "    return data  #data是矩阵   （batchsize，length/batchsize)\n",
    "\n",
    "\n",
    "def getBatch(data, seq_length):\n",
    "    #data是矩阵:（batchsize，length/batchsize)           #data.size() = torch.Size([16, 58099]) \n",
    "    #inputs,targets都是矩阵，target是inputs的下一个单词   （batch，seq_length) =(16,30)\n",
    "     for i in range(0, data.size(1) - seq_length, seq_length):\n",
    "        inputs = Variable(data[:, i: i + seq_length])\n",
    "        targets = Variable(data[:, (i + 1): (i + 1) + seq_length].contiguous())\n",
    "        yield (inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size,= 10000\ntorch.Size([929589])\ntorch.Size([16, 58099])\n"
     ]
    }
   ],
   "source": [
    "train_data, word2index = prepare_ptb_dataset(r'C:\\workspace\\python-work\\nlp\\NLP-cs224n\\NLP-cs224n_ec\\notebooks\\dataset\\ptb\\ptb.train.txt',)\n",
    "dev_data , _ = prepare_ptb_dataset(r'C:\\workspace\\python-work\\nlp\\NLP-cs224n\\NLP-cs224n_ec\\notebooks\\dataset\\ptb\\ptb.valid.txt', word2index)\n",
    "test_data, _ = prepare_ptb_dataset(r'C:\\workspace\\python-work\\nlp\\NLP-cs224n\\NLP-cs224n_ec\\notebooks\\dataset\\ptb\\ptb.test.txt', word2index)\n",
    "\n",
    "print('vocab_size,=',len(word2index))\n",
    "index2word = {v:k for k, v in word2index.items()}\n",
    "\n",
    "\n",
    "EMBED_SIZE = 128           #d\n",
    "HIDDEN_SIZE = 256          #h\n",
    "NUM_LAYER = 1              #l\n",
    "LR = 0.01\n",
    "SEQ_LENGTH = 30            # max_length       for BPTT（Back Propagation Trough Time） \n",
    "BATCH_SIZE = 16            #b\n",
    "EPOCH = 1\n",
    "RESCHEDULED = False\n",
    "\n",
    "print(train_data.size())\n",
    "#train_data是矩阵:（batchsize，length/batchsize)             不同行的word已经混合了，然后统一分为batchsize份\n",
    "train_data = batchify(train_data, BATCH_SIZE)    \n",
    "print(train_data.size())\n",
    "# torch.Size([929589])\n",
    "# torch.Size([16, 58099])            16*58099 = 929584\n",
    "dev_data = batchify(dev_data, BATCH_SIZE//2)\n",
    "test_data = batchify(test_data, BATCH_SIZE//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LanguageModel(\n  (embed): Embedding(10000, 128)\n  (LSTM): LSTM(128, 256, batch_first=True)\n  (linear): Linear(in_features=256, out_features=10000, bias=True)\n  (dropout): Dropout(p=0.5)\n)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\software_installed_cs\\Anaconda3\\envs\\pytorch0.4\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n  del sys.path[0]\nC:\\software_installed_cs\\Anaconda3\\envs\\pytorch0.4\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n  \nC:\\software_installed_cs\\Anaconda3\\envs\\pytorch0.4\\lib\\site-packages\\ipykernel_launcher.py:79: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\software_installed_cs\\Anaconda3\\envs\\pytorch0.4\\lib\\site-packages\\ipykernel_launcher.py:81: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/1] mean_loss : 7.36, Perplexity : 1567.21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/1] mean_loss : 6.66, Perplexity : 781.53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/1] mean_loss : 6.58, Perplexity : 719.81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/1] mean_loss : 6.46, Perplexity : 639.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/1] mean_loss : 6.58, Perplexity : 723.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/1] mean_loss : 6.41, Perplexity : 610.24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/1] mean_loss : 6.40, Perplexity : 599.53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/1] mean_loss : 6.21, Perplexity : 499.29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/1] mean_loss : 6.23, Perplexity : 506.44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/1] mean_loss : 6.05, Perplexity : 424.70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/1] mean_loss : 6.09, Perplexity : 439.51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/1] mean_loss : 6.08, Perplexity : 435.39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/1] mean_loss : 5.89, Perplexity : 360.59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/1] mean_loss : 5.97, Perplexity : 389.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/1] mean_loss : 5.94, Perplexity : 381.54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/1] mean_loss : 5.97, Perplexity : 392.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/1] mean_loss : 5.87, Perplexity : 353.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/1] mean_loss : 5.85, Perplexity : 348.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/1] mean_loss : 5.81, Perplexity : 335.07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/1] mean_loss : 5.83, Perplexity : 341.57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/1] mean_loss : 5.69, Perplexity : 295.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/1] mean_loss : 5.80, Perplexity : 330.38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/1] mean_loss : 5.70, Perplexity : 298.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/1] mean_loss : 5.69, Perplexity : 297.33\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/1] mean_loss : 5.63, Perplexity : 278.79\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/1] mean_loss : 5.68, Perplexity : 292.61\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/1] mean_loss : 5.65, Perplexity : 285.66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00/1] mean_loss : 5.70, Perplexity : 297.77\n"
     ]
    }
   ],
   "source": [
    "class LanguageModel(nn.Module): \n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, n_layers=1, dropout_p=0.5):\n",
    "\n",
    "        super(LanguageModel, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_size)  #(d,v)\n",
    "        self.LSTM = nn.LSTM(embedding_size, hidden_size, n_layers, batch_first=True)      #(h,d)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)  #(h,v)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "    def init_weight(self):\n",
    "        self.embed.weight = nn.init.xavier_uniform(self.embed.weight)\n",
    "        self.linear.weight = nn.init.xavier_uniform(self.linear.weight)\n",
    "        self.linear.bias.data.fill_(0)\n",
    "        \n",
    "    def init_hidden(self,batch_size):\n",
    "        attention = Variable(torch.zeros(self.n_layers,batch_size,self.hidden_size))\n",
    "        cell = Variable(torch.zeros(self.n_layers,batch_size,self.hidden_size))\n",
    "        return  (attention, cell)\n",
    "    \n",
    "    #保持原网络参数不变，只训练部分分支，或少数几层网络；切断这些分支的反向传播\n",
    "    def detach_hidden(self, hiddens):\n",
    "        return tuple([hidden.detach() for hidden in hiddens])\n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward(self, inputs, hidden, is_training=False): \n",
    "\n",
    "        embeds = self.embed(inputs)   #(d,v) * (v,b,seq_length) = （b,seq_length，d)=torch.Size([16, 30, 128])\n",
    "        if is_training:\n",
    "            embeds = self.dropout(embeds)\n",
    "        # attention_state = Variable(torch.zeros(1, len(X), n_hidden))           # [num_layers(=1) * num_directions(=1), b, h]=(1,b,h)\n",
    "        # cell_state = Variable(torch.zeros(1, len(X), n_hidden))              # [num_layers(=1) * num_directions(=1), b, h]=(1,b,h)\n",
    "        #hidden = (attention_state,cell_state)\n",
    "        out,hidden = self.LSTM(embeds, hidden)            #  self.lstm(   (m,b,d) , ( (1,b,h),(1,b,h)  )  \n",
    "        # out.size() = (b,seq_length,h) \n",
    "        # hidden = (hidden_state,cell_state) =  ( (1,b,h) ,(1,b,h)) \n",
    "        #第一个返回值(v,h) * (h,b) *= (v,b) 然后softmax，        第二个返回值（h，b)\n",
    "        return self.linear(out.contiguous().view(out.size(0) * out.size(1), -1)), hidden    \n",
    "        #(h,v) * (b*seq_length,h) = (b*seq_lenth,v) ==========> (b*seq_length,v) = (16*30,1000) =  torch.Size([480, 10000])\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "model = LanguageModel(len(word2index), EMBED_SIZE, HIDDEN_SIZE, NUM_LAYER, 0.5)\n",
    "print(model)\n",
    "'''\n",
    "LanguageModel(\n",
    "  (embed): Embedding(10000, 128)\n",
    "  (rnn): LSTM(128, 256, batch_first=True)\n",
    "  (linear): Linear(in_features=256, out_features=10000, bias=True)\n",
    "  (dropout): Dropout(p=0.5)\n",
    ")\n",
    "'''\n",
    "model.init_weight() \n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    total_loss = 0\n",
    "    losses = []\n",
    "    hidden = model.init_hidden(BATCH_SIZE)  #hidden是元组 (attention,cell)相当于初始化的attention，cell    hidden[0].size()=torch.Size([1, 16, 256])=（1，b，h)\n",
    "    #train_data是矩阵:（batchsize，length/batchsize)    #SEQ_LENGTH = 30            # for BPTT（Back Propagation Trough Time）\n",
    "    for i,batch in enumerate(getBatch(train_data, SEQ_LENGTH)):   \n",
    "    \n",
    "        # inputs.size()=target.size()  = (b,seq_length)= torch.Size([16, 30]) \n",
    "        inputs, targets = batch\n",
    "        #切断这些分支的反向传播,保持原网络参数不变\n",
    "        hidden = model.detach_hidden(hidden)\n",
    "        model.zero_grad()\n",
    "        preds, hidden = model(inputs, hidden, True)\n",
    "        #preds.size() = (b*seq_length,v) = (16*30,1000) \n",
    "        loss = loss_function(preds, targets.view(-1))     #(v,b,seq_length) 与（v,b,seq_length）之间的softmax\n",
    "        losses.append(loss.data[0])\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), 0.5) # gradient clipping\n",
    "        optimizer.step()\n",
    "\n",
    "        if i > 0 and i % 50 == 0:\n",
    "            print(\"[%02d/%d] mean_loss : %0.2f, Perplexity : %0.2f\" % (epoch,EPOCH, np.mean(losses), np.exp(np.mean(losses))))\n",
    "            losses = []\n",
    "        \n",
    "\n",
    "    if RESCHEDULED == False and epoch == EPOCH//2:\n",
    "            LR *= 0.1\n",
    "            optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "            RESCHEDULED = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Perpelexity : 9812.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\software_installed_cs\\Anaconda3\\envs\\pytorch0.4\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "total_loss = 0\n",
    "hidden = model.init_hidden(BATCH_SIZE//2)\n",
    "for batch in getBatch(test_data, SEQ_LENGTH):\n",
    "    inputs,targets = batch\n",
    "    hidden = model.detach_hidden(hidden)\n",
    "    model.zero_grad()\n",
    "    preds, hidden = model(inputs, hidden)\n",
    "    total_loss += inputs.size(1) * loss_function(preds, targets.view(-1)).data\n",
    "\n",
    "total_loss = total_loss[0]/test_data.size(1)\n",
    "print(\"Test Perpelexity : %5.2f\" % (np.exp(total_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
