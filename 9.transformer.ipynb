{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#referencing paper \"Attention is all you need\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dtype = torch.FloatTensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5]) torch.Size([1, 5]) torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "# S: Symbol that shows starting of decoding input\n",
    "# E: Symbol that shows starting of decoding output\n",
    "# P: Symbol that will fill in blank sequence if current batch data size is short than time \n",
    "\n",
    "sentences = ['ich mochte ein bier P', 'S i want a beer', 'i want a beer E']\n",
    "\n",
    "# Transformer Parameters\n",
    "# Padding Should be Zero\n",
    "\n",
    "src_vocab = {'P' : 0, 'ich' : 1, 'mochte' : 2, 'ein' : 3, 'bier' : 4}\n",
    "src_vocab_size = len(src_vocab)       #src_vocab_size=v_x=5\n",
    "\n",
    "tgt_vocab = {'P' : 0, 'i' : 1, 'want' : 2, 'a' : 3, 'beer' : 4, 'S' : 5, 'E' : 6}\n",
    "number_dict = {i: w for i, w in enumerate(tgt_vocab)}\n",
    "tgt_vocab_size = len(tgt_vocab)       #tgt_vocab_size=v_y=7\n",
    "\n",
    "src_len = 5      #m_x\n",
    "tgt_len = 5      #m_y\n",
    "\n",
    "d_model = 512  # Embedding Size=d\n",
    "d_ff = 2048    # dimension  of feedforward      等于Conv1d操作中的output_channel，等于Conv1d操作中的number_of_filter\n",
    "d_k = d_v = 64  # dimension of K(=Q), V    =d_model/n_head=512/8=64\n",
    "n_layers = 6  # number of Encoder of Decoder Layer\n",
    "n_heads = 8  # number of heads in Multi-Head Attention\n",
    "\n",
    "\n",
    "def make_batch(sentences):\n",
    "    input_batch = [[src_vocab[n] for n in sentences[0].split()]]\n",
    "    output_batch = [[tgt_vocab[n] for n in sentences[1].split()]]\n",
    "    target_batch = [[tgt_vocab[n] for n in sentences[2].split()]]\n",
    "    return Variable(torch.LongTensor(input_batch)), Variable(torch.LongTensor(output_batch)), Variable(torch.LongTensor(target_batch))\n",
    "\n",
    "enc_inputs, dec_inputs, target_batch = make_batch(sentences)\n",
    "# enc_inputs: 'ich mochte ein bier P'======>[1,2,3,4,0]\n",
    "# dec_inputs:  'S i want a beer'===========>[5,1,2,3,4]\n",
    "# target_batch:'i want a beer E'===========>[1,2,3,4,6]\n",
    "print(enc_inputs.size(),dec_inputs.size(),target_batch.size())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_sinusoid_encoding_table()  返回(m_x+1,d)的矩阵\n",
    "def get_sinusoid_encoding_table(n_position, d_model):      #(m_x+1,d)           得到正弦曲线 编码  表              positional encoding\n",
    "    def cal_angle(position, hid_idx):\n",
    "        return position / np.power(10000, 2 * (hid_idx // 2) / d_model)\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, hid_j) for hid_j in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(n_position)])\n",
    "    # print(sinusoid_table.shape)    #(m_x,d) = (6,512)\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])    # 当hid_idx为偶数时： =sin（x）\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])    # 当hid_idx为奇数时：  =cos（x）\n",
    "    return torch.FloatTensor(sinusoid_table)\n",
    "\n",
    "    # sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in r\n",
    "\n",
    "\n",
    "def get_attn_pad_mask(seq_q, seq_k):         #(b,m_x) ,(b,m_x)\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    # eq(zero) is PAD token\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)       #(b,m_x)===>(b,1,m_x)    one is masking\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # (b,m_q,m_k)=(b,m_x,m_x)     batch_size x len_q x len_k\n",
    "\n",
    "\n",
    "def get_attn_subsequent_mask(seq):                 # (b,m_y) \n",
    "    attn_shape = [seq.size(0), seq.size(1), seq.size(1)]   #(b,m_y,m_y)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1)\n",
    "    subsequent_mask = torch.from_numpy(subsequent_mask).byte()\n",
    "    return subsequent_mask       #(b,m_y,m_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):       #forward(  (b,8,m_q,d_k), (b,8,m_,d_k), (b,8,m_,d_k), (b,8,m_q,m_k)   ) \n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : (b,8,m_q,m_q)\n",
    "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
    "        attn = nn.Softmax(dim=-1)(scores)     #(b,8,m_q,m_q)\n",
    "        context = torch.matmul(attn, V)       #(b,8,m_q,m_k)*(b,8,m_v,d_v)=(b,8,m_q,d_v)\n",
    "        return context, attn         #(b,8,m_q,d_v)   (b,8,m_q,m_q)          \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads) #(d,8*d_k)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
    "    def forward(self, Q, K, V, attn_mask):          #forward(  (b,m_q,d),(b,m_k,d),(b,m_k,d),   (b,m_q,m_k) )       \n",
    "        residual, batch_size = Q, Q.size(0)      #residual:(b,m_q,d)  是进行multi_head_attettion操作之前copy的数据，等于enc_outputs\n",
    "        #(8d_q,d) * (b,m_q,d)=(8*d_q,b,m_q)====>(b,m_q,8,d_k)=====>(b,8,m_q,d_q)\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: (b,8,m_q,d_q)\n",
    "        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s:(b,8,m_k,d_k)\n",
    "        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: (b,8,m_k,d_k)\n",
    "       \n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1)                # attn_mask :  (b,m_q,m_k) ===>(b,1,m_q,m_k)==>(b,8,m_q,m_k)\n",
    "        context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)      # context:(b,8,m_q,d_v)         attn: (b,8,m_x,m_x) \n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v)  #context:(b,8,m_q,d_v) ==>(b,m_q,8,d_v)====>(b,m_q,8*d_v)\n",
    "        output = nn.Linear(n_heads * d_v, d_model)(context)                        #ouput:(8*d_v,d)*(b,m_q,8*d_k)=(b,m_q,d)\n",
    "        return nn.LayerNorm(d_model)(output + residual), attn                      #  return:(b,m_q,d), (b,8,m_x,m_x) \n",
    "\n",
    "\n",
    "\n",
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n",
    "\n",
    "    def forward(self, inputs):       # (b,m_x,d)         #(b,d，m_x) 运行conv1操作（把d当做input_channel）,得到(b,output_channle,m_x)\n",
    "        residual = inputs    #residual: (b,m_x,d) \n",
    "        inputs=inputs.transpose(1,2)  # (b,m_x,d)========>#(b,input_channel(=d),m_x)  \n",
    "        inputs=self.conv1(inputs)    #(b,input_channel(=d),m_x)----->在input_channel方向做卷积操作------>(b,output_channle(=d_ff),m_x)=(1,2028，5)\n",
    "        output = nn.ReLU()(inputs)  \n",
    "        \n",
    "        #(b,input_channel(=d_ff),m_x)-->在input_channel方向做卷积操作-->(b,output_channle(=d_model),m_x)--->(b,m_x,d)=(1,5,512)\n",
    "        output = self.conv2(output).transpose(1, 2) \n",
    "        \n",
    "        return nn.LayerNorm(d_model)(output + residual)  #return (b,m_x,d)\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = PoswiseFeedForwardNet()             #position_feed_forward_network\n",
    "\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):       #forward(  (b,m_x,d),(b,m_q,m_k) )\n",
    "                          # self.enc_self_attn(  (b,m_x,d),(b,m_x,d),(b,m_x,d),   (b,m_q,m_k) )         \n",
    "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_outputs:(b,m_x,d)  attn:(b,8,m_x,m_x)\n",
    "        enc_outputs = self.pos_ffn(enc_outputs) # self.pos_ffn( (b,m_x,d) )======>(b,m_x,d)\n",
    "        return enc_outputs, attn         #(b,m_x,d)   (b,8,m_x,m_x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.dec_self_attn = MultiHeadAttention()\n",
    "        self.dec_enc_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = PoswiseFeedForwardNet()         # position_feed_forward_network\n",
    "\n",
    "    def forward(self, dec_inputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask): #forward(  (b,m_y,d),(b,m_x,d), (b,m_y,m_y),(b,m_x,m_y)    )\n",
    "                                   # self.enc_self_attn(  (b,m_y,d),(b,m_y,d),(b,m_y,d),   (b,m_y,m_y) ) \n",
    "        dec_outputs, dec_self_attn = self.dec_self_attn(dec_inputs, dec_inputs, dec_inputs, dec_self_attn_mask)#dec_outputs:(b,m_y,d) attn:(b,8,m_y,m_y) \n",
    "        #dec_outputs作为q，enc_outpus作为k和v    #self.enc_self_attn(  (b,m_y,d),(b,m_x,d),(b,m_x,d),   (b,m_y,m_y) ) = self.dec_enc_atten( Q, K, V,attn_mask) \n",
    "        dec_outputs, dec_enc_attn = self.dec_enc_attn(dec_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)#dec_outputs:(b,m_y,d) attn:(b,8,m_y,m_y) \n",
    "        dec_outputs = self.pos_ffn(dec_outputs)     # self.pos_ffn( (b,m_y,d) )======>(b,m_y,d)                            \n",
    "        return dec_outputs, dec_self_attn, dec_enc_attn        # (b,m_y,d)   (b,8,m_y,m_y)     (b,8,m_x,m_y)\n",
    " \n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.src_emb = nn.Embedding(src_vocab_size, d_model)   #(v_x,d)\n",
    "        self.pos_emb = nn.Embedding.from_pretrained(get_sinusoid_encoding_table(src_len+1, d_model),freeze=True)  #(m_x+1,d)\n",
    "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, enc_inputs): # enc_inputs:(b,m_x)  \n",
    "        # print(self.src_emb(enc_inputs).size())                                #(v_x,d)  *  (b,m_x) = (b,m_x,d)\n",
    "        # print(self.pos_emb(torch.LongTensor([[1,2,3,4,0]])).size())           #(m_x,d)* (b,m_x) = (b,m_x,d)\n",
    "        enc_outputs = self.src_emb(enc_inputs) + self.pos_emb(torch.LongTensor([[1,2,3,4,0]]))      #(b,m_x,d) + (b,m_x,d) =(b,m_x,d) \n",
    "        enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs)            #get_attn_pad_mask(  (b,m_x) ,(b,m_x)  )  ======>(b,m_q,m_k)\n",
    "        enc_self_attns = []\n",
    "        for layer in self.layers:     #n_layers(=6)个一样的encoder layer       每一次做一次迭代\n",
    "            # enc_outputs:(b,m_x,d)        enc_self_attn:(b,8,m_x,m_x)\n",
    "            enc_outputs, enc_self_attn = layer(enc_outputs, enc_self_attn_mask)         #layer(  (b,m_x,d),(b,m_q,m_k) )\n",
    "            enc_self_attns.append(enc_self_attn)\n",
    "        return enc_outputs, enc_self_attns      # enc_outputs:(b,m_x,d)        enc_self_attns是数组，每个元素：(b,8,m_x,m_x)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)   #(v_y,d)\n",
    "        self.pos_emb = nn.Embedding.from_pretrained(get_sinusoid_encoding_table(tgt_len+1, d_model),freeze=True)\n",
    "        self.layers = nn.ModuleList([DecoderLayer() for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, dec_inputs, enc_inputs, enc_outputs):          #forward( (b,m_y),(b,m_x),(b,m_x,d) )                      \n",
    "        dec_outputs = self.tgt_emb(dec_inputs) + self.pos_emb(torch.LongTensor([[5,1,2,3,4]]))        #(b,m_y,d) + (b,m_y,d) = (b,m_y,d) \n",
    "        \n",
    "        dec_self_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs)    #get_attn_pad_mask(  (b,m_y) ,(b,m_y)  )  ======>(b,m_q,m_k)\n",
    "        dec_self_attn_subsequent_mask = get_attn_subsequent_mask(dec_inputs)   #get_attn_subsequent_mask( (b,m_y) )======>(b,m_y,m_y)\n",
    "        dec_self_attn_mask = torch.gt( (dec_self_attn_pad_mask + dec_self_attn_subsequent_mask), 0  )  #(b,m_y,m_y)\n",
    "        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs)           ##(b,m_x,m_y)  \n",
    "\n",
    "        dec_self_attns, dec_enc_attns = [], []\n",
    "        for layer in self.layers:               #n_layers(=6)个一样的decoder layer       每一次做一次迭代\n",
    "                                                      #layer(  (b,m_y,d),(b,m_x,d), (b,m_y,m_y),(b,m_x,m_y)    )\n",
    "            dec_outputs, dec_self_attn, dec_enc_attn = layer(dec_outputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask)\n",
    "            # enc_outputs:(b,m_y,d)        dec_self_attn: (b,8,m_y,m_y)      dec_enc_attn:(b,8,m_x,m_y)\n",
    "                                                      \n",
    "            dec_self_attns.append(dec_self_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "        return dec_outputs, dec_self_attns, dec_enc_attns    # dec_outputs:(b,m_y,d)     dec_self_attns和dec_enc_attns是数组，每个元素：(b,8,m_y,m_y)\n",
    "\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        self.projection = nn.Linear(d_model, tgt_vocab_size, bias=False)    #(d,v_y)\n",
    "    def forward(self, enc_inputs, dec_inputs):              #forward (  (b,m_x ),(b,m_y)  )   \n",
    "         # enc_outputs:(b,m_x,d)        enc_self_attns是数组，每个元素：(b,8,m_x,m_x)\n",
    "        enc_outputs, enc_self_attns = self.encoder(enc_inputs)        #(b,m_x)\n",
    "         \n",
    "         #dec_outputs:(b,m_y,d)     dec_self_attns和dec_enc_attns是数组，每个元素：(b,8,m_y,m_y)\n",
    "        dec_outputs, dec_self_attns, dec_enc_attns = self.decoder(dec_inputs, enc_inputs, enc_outputs)   #self.decoder( (b,m_y),(b,m_x),(b,m_x,d) )\n",
    "         \n",
    "        dec_logits = self.projection(dec_outputs)     #(d,v_y)* (b,m_y,d) = (b,m_y,v_y) \n",
    "        dec_logits = dec_logits.view(-1, dec_logits.size(-1)) #(b,m_y,v_y)------->(b*m_y,v_y)\n",
    "         \n",
    "        #dec_logits：(b*m_y,v_y)      enc_self_attn和sec_self_attns和dec_enc_attns是数组，每个元素：(b,8,m,m)\n",
    "        return  dec_logits,enc_self_attns, dec_self_attns, dec_enc_attns    \n",
    "      \n",
    "\n",
    "\n",
    "def showgraph(attn):\n",
    "    attn = attn[-1].squeeze(0)[0]\n",
    "    attn = attn.squeeze(0).data.numpy()\n",
    "    fig = plt.figure(figsize=(n_heads, n_heads)) # [n_heads, n_heads]\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attn, cmap='viridis')\n",
    "    ax.set_xticklabels(['']+sentences[0].split(), fontdict={'fontsize': 14}, rotation=90)\n",
    "    ax.set_yticklabels(['']+sentences[2].split(), fontdict={'fontsize': 14})\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTransformer(\\n\\n  (encoder): Encoder(\\n    (src_emb): Embedding(5, 512)\\n    (pos_emb): Embedding(6, 512)\\n    (layers): ModuleList(\\n    \\n      (0-5): EncoderLayer(            #6个一样的encoder layer\\n        (enc_self_attn): MultiHeadAttention(\\n          (W_Q): Linear(in_features=512, out_features=512, bias=True)\\n          (W_K): Linear(in_features=512, out_features=512, bias=True)\\n          (W_V): Linear(in_features=512, out_features=512, bias=True)\\n        )\\n        (pos_ffn): PoswiseFeedForwardNet(\\n          (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))\\n          (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))\\n        )\\n      )\\n\\n    )\\n  )\\n  \\n  (decoder): Decoder(                 #6个一样的decoder layer\\n    (tgt_emb): Embedding(7, 512)\\n    (pos_emb): Embedding(6, 512)\\n    (layers): ModuleList(\\n    \\n      (0-5): DecoderLayer(\\n        (dec_self_attn): MultiHeadAttention(\\n          (W_Q): Linear(in_features=512, out_features=512, bias=True)\\n          (W_K): Linear(in_features=512, out_features=512, bias=True)\\n          (W_V): Linear(in_features=512, out_features=512, bias=True)\\n        )\\n        (dec_enc_attn): MultiHeadAttention(\\n          (W_Q): Linear(in_features=512, out_features=512, bias=True)\\n          (W_K): Linear(in_features=512, out_features=512, bias=True)\\n          (W_V): Linear(in_features=512, out_features=512, bias=True)\\n        )\\n        (pos_ffn): PoswiseFeedForwardNet(\\n          (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))\\n          (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))\\n        )\\n      )\\n      \\n    )\\n  )\\n  \\n  \\n  (projection): Linear(in_features=512, out_features=7, bias=False)\\n)\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "optimizer = optim.Adam(    filter(lambda p: p.requires_grad,model.parameters()),   lr=0.001)\n",
    "'''\n",
    "Transformer(\n",
    "\n",
    "  (encoder): Encoder(\n",
    "    (src_emb): Embedding(5, 512)\n",
    "    (pos_emb): Embedding(6, 512)\n",
    "    (layers): ModuleList(\n",
    "    \n",
    "      (0-5): EncoderLayer(            #6个一样的encoder layer\n",
    "        (enc_self_attn): MultiHeadAttention(\n",
    "          (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
    "          (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
    "          (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
    "        )\n",
    "        (pos_ffn): PoswiseFeedForwardNet(\n",
    "          (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))\n",
    "          (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))\n",
    "        )\n",
    "      )\n",
    "\n",
    "    )\n",
    "  )\n",
    "  \n",
    "  (decoder): Decoder(                 #6个一样的decoder layer\n",
    "    (tgt_emb): Embedding(7, 512)\n",
    "    (pos_emb): Embedding(6, 512)\n",
    "    (layers): ModuleList(\n",
    "    \n",
    "      (0-5): DecoderLayer(\n",
    "        (dec_self_attn): MultiHeadAttention(\n",
    "          (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
    "          (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
    "          (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
    "        )\n",
    "        (dec_enc_attn): MultiHeadAttention(\n",
    "          (W_Q): Linear(in_features=512, out_features=512, bias=True)\n",
    "          (W_K): Linear(in_features=512, out_features=512, bias=True)\n",
    "          (W_V): Linear(in_features=512, out_features=512, bias=True)\n",
    "        )\n",
    "        (pos_ffn): PoswiseFeedForwardNet(\n",
    "          (conv1): Conv1d(512, 2048, kernel_size=(1,), stride=(1,))\n",
    "          (conv2): Conv1d(2048, 512, kernel_size=(1,), stride=(1,))\n",
    "        )\n",
    "      )\n",
    "      \n",
    "    )\n",
    "  )\n",
    "  \n",
    "  \n",
    "  (projection): Linear(in_features=512, out_features=7, bias=False)\n",
    ")\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  3,  4,  0]]) tensor([[ 5,  1,  2,  3,  4]]) tensor([[ 1,  2,  3,  4,  6]])\ntorch.Size([1, 5]) torch.Size([1, 5]) torch.Size([1, 5])\nEpoch: 0001 cost = 1.859222\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    optimizer.zero_grad()\n",
    "    enc_inputs, dec_inputs, target_batch = make_batch(sentences)        \n",
    "    print(enc_inputs,dec_inputs,target_batch)\n",
    "    print(enc_inputs.size(),dec_inputs.size(),target_batch.size())\n",
    "    \n",
    "    #output：(b*m_y,v_y)      enc_self_attn和sec_self_attns和dec_enc_attns是数组，每个元素：(b,8,m,m)\n",
    "    outputs, enc_self_attns, dec_self_attns, dec_enc_attns = model(enc_inputs, dec_inputs)   #model( (b,m_x ),(b,m_y)  )\n",
    "    target=target_batch.contiguous().view(-1)      #  (b*m_y)\n",
    "    loss = criterion(outputs,target)   #criterion(  (b*m_y,v_y), (b*m_y)  )\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ich mochte ein bier P -> ['i', 'want', 'a', 'beer', 'E']\nfirst head of last state enc_self_attns\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first head of last state dec_self_attns\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first head of last state dec_enc_attns\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test\n",
    "predict, _, _, _ = model(enc_inputs, dec_inputs)\n",
    "predict = predict.data.max(1, keepdim=True)[1]\n",
    "print(sentences[0], '->', [number_dict[n.item()] for n in predict.squeeze()])\n",
    "\n",
    "print('first head of last state enc_self_attns')\n",
    "showgraph(enc_self_attns)\n",
    "\n",
    "print('first head of last state dec_self_attns')\n",
    "showgraph(dec_self_attns)\n",
    "\n",
    "print('first head of last state dec_enc_attns')\n",
    "showgraph(dec_enc_attns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
