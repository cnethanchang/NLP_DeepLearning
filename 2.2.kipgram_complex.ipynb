{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import  torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "random.seed(1024)\n",
    "\n",
    "FloatTensor = torch.FloatTensor\n",
    "LongTensor = torch.LongTensor\n",
    "ByteTensor = torch.ByteTensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n 'austen-persuasion.txt',\n 'austen-sense.txt',\n 'bible-kjv.txt',\n 'blake-poems.txt',\n 'bryant-stories.txt',\n 'burgess-busterbrown.txt',\n 'carroll-alice.txt',\n 'chesterton-ball.txt',\n 'chesterton-brown.txt',\n 'chesterton-thursday.txt',\n 'edgeworth-parents.txt',\n 'melville-moby_dick.txt',\n 'milton-paradise.txt',\n 'shakespeare-caesar.txt',\n 'shakespeare-hamlet.txt',\n 'shakespeare-macbeth.txt',\n 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[', 'Moby', 'Dick', 'by', 'Herman', 'Melville', '1851', ']'],\n ['ETYMOLOGY', '.'],\n ['(',\n  'Supplied',\n  'by',\n  'a',\n  'Late',\n  'Consumptive',\n  'Usher',\n  'to',\n  'a',\n  'Grammar',\n  'School',\n  ')']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.sents('melville-moby_dick.txt')[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'moby', 'dick', 'by', 'herman', 'melville', '1851', ']']\n['etymology', '.']\n['(', 'supplied', 'by', 'a', 'late', 'consumptive', 'usher', 'to', 'a', 'grammar', 'school', ')']\n"
     ]
    }
   ],
   "source": [
    "corpus = list(nltk.corpus.gutenberg.sents('melville-moby_dick.txt'))[:100] # sampling sentences for test\n",
    "corpus = [  [word.lower() for word in sent] for sent in corpus  ]\n",
    "for i in range(3):\n",
    "    print(corpus[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1463\n592\n5\n"
     ]
    }
   ],
   "source": [
    "flatten = lambda l: [item    for sublist in l     for item in sublist]\n",
    "#flattern is a functionword_count = Counter(flatten(corpus))\n",
    "#faltten是匿名函数，三维变为一维\n",
    "\n",
    "print(len(flatten(corpus)))\n",
    "\n",
    "word_count = Counter(flatten(corpus))\n",
    "print(len(word_count))\n",
    "#共有592个不同的单词/word\n",
    "# print(word_count)\n",
    "# Counter({',': 96, '.': 66, 'the': 58, 'of': 36,  'man': 1})\n",
    "border = int(len(word_count)*0.01) \n",
    "print(border)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 96), ('.', 66), ('the', 58), ('of', 36), ('and', 35), ('man', 1), ('artificial', 1), ('civitas', 1), ('--(', 1), ('state', 1)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[',', '.', 'the', 'of', 'and', 'man', 'artificial', 'civitas', '--(', 'state']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(  type(word_count.most_common()  ))\n",
    " # [(',', 96),('.', 66),('the', 58),('of', 36), ('and', 35)......,('man', 1)]\n",
    "\n",
    "stopwords_first = word_count.most_common()[:border] \n",
    "stopwords_last = list(reversed(word_count.most_common()))[:border]\n",
    "stopwords = stopwords_first + stopwords_last\n",
    "print(stopwords)\n",
    "stopwords = [ s[0] for s in stopwords]\n",
    "stopwords\n",
    "#stopwords 是corpus中个数最多和 最少的几个单词\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "592 583\n"
     ]
    }
   ],
   "source": [
    "#vocab为 减去最多和最少频率的单词 之后的list\n",
    "vocab= list(  set(flatten(corpus))-set(stopwords)   )\n",
    "vocab.append('<UNK>')\n",
    "# print(vocab)\n",
    "print(len(set(flatten(corpus))), len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#最后得到的  word2index  为除去减去最多和最少频率的单词之后的  {'UNK':0,单词：1，单词：2...}\n",
    "# 最后得到的  index2word  为除去减去最多和最少频率的单词之后的 {0:'UNK',1:单词，2:单词....}\n",
    "\n",
    "word2index = {'<UNK>':0}\n",
    "for vo in vocab:\n",
    "    if word2index.get(vo) is None:\n",
    "        word2index[vo] = len(word2index)\n",
    "index2word = { v:k for k,v in  word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<DUMMY>', '<DUMMY>', '<DUMMY>', '[', 'moby', 'dick', 'by')\n('<DUMMY>', '<DUMMY>', '[', 'moby', 'dick', 'by', 'herman')\n"
     ]
    }
   ],
   "source": [
    "# print(corpus)\n",
    "'''\n",
    "[   [   '[', 'moby', 'dick', 'by', 'herman', 'melville', '1851', ']'], \n",
    "    ['etymology', '.'], \n",
    "    ['(', 'supplied', 'by', 'a', 'late', 'consumptive', 'usher', 'to', 'a', 'grammar', 'school', ')'],\n",
    "    ['the', 'pale', 'usher', '--', 'threadbare', 'in', 'coat', ',', 'see', 'him', 'now', '.']]\n",
    "'''\n",
    "#从corpus中取wind_size=3 的滑动窗口\n",
    "WINDOW_SIZE = 3\n",
    "windows = flatten(  [list(    nltk.ngrams(  ['<DUMMY>'] * WINDOW_SIZE + c + ['<DUMMY>'] * WINDOW_SIZE, WINDOW_SIZE * 2 + 1  )                ) \n",
    "                          for c in corpus  ]   \n",
    "                 )\n",
    "# print(windows)\n",
    "'''\n",
    "[\t\t('<DUMMY>', '<DUMMY>', '<DUMMY>', '[', 'moby', 'dick', 'by'), \n",
    "\t\t('<DUMMY>', '<DUMMY>', '[', 'moby', 'dick', 'by', 'herman'), \n",
    "\t\t('<DUMMY>', '[', 'moby', 'dick', 'by', 'herman', 'melville'), \n",
    "\t\t('[', 'moby', 'dick', 'by', 'herman', 'melville', '1851'), \n",
    "\t\t('moby', 'dick', 'by', 'herman', 'melville', '1851', ']'), \n",
    "\t\t('dick', 'by', 'herman', 'melville', '1851', ']', '<DUMMY>'), \n",
    "\t\t('by', 'herman', 'melville', '1851', ']', '<DUMMY>', '<DUMMY>'), \n",
    "\t\t('herman', 'melville', '1851', ']', '<DUMMY>', '<DUMMY>', '<DUMMY>'), \n",
    "\t\t\n",
    "\t\t('<DUMMY>', '<DUMMY>', '<DUMMY>', 'etymology', '.', '<DUMMY>', '<DUMMY>'), \n",
    "\t\t('<DUMMY>', '<DUMMY>', 'etymology', '.', '<DUMMY>', '<DUMMY>', '<DUMMY>'), \n",
    "\t\t\n",
    "\t\t('<DUMMY>', '<DUMMY>', '<DUMMY>', '(', 'supplied', 'by', 'a'), \n",
    "\t\t('<DUMMY>', '<DUMMY>', '(', 'supplied', 'by', 'a', 'late'), \n",
    "\t\t('<DUMMY>', '(', 'supplied', 'by', 'a', 'late', 'consumptive'), \n",
    "\t\t('(', 'supplied', 'by', 'a', 'late', 'consumptive', 'usher'), \n",
    "\t\t('supplied', 'by', 'a', 'late', 'consumptive', 'usher', 'to'), \n",
    "\t\t('by', 'a', 'late', 'consumptive', 'usher', 'to', 'a'), \n",
    "\t\t('a', 'late', 'consumptive', 'usher', 'to', 'a', 'grammar'), \n",
    "\t\t('late', 'consumptive', 'usher', 'to', 'a', 'grammar', 'school'), \n",
    "\t\t('consumptive', 'usher', 'to', 'a', 'grammar', 'school', ')'), \n",
    "\t\t('usher', 'to', 'a', 'grammar', 'school', ')', '<DUMMY>'), \n",
    "\t\t('to', 'a', 'grammar', 'school', ')', '<DUMMY>', '<DUMMY>'), \n",
    "\t\t('a', 'grammar', 'school', ')', '<DUMMY>', '<DUMMY>', '<DUMMY>')\n",
    "\t\t............................]\n",
    "\n",
    "'''\n",
    "print(windows[0])\n",
    "print(windows[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('[', 'moby'), ('[', 'dick'), ('[', 'by'), ('moby', '['), ('moby', 'dick'), ('moby', 'by'), ('moby', 'herman'), ('dick', '['), ('dick', 'moby'), ('dick', 'by')]\n475\n"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "\n",
    "for window in windows:\n",
    "    for i in range(WINDOW_SIZE * 2 + 1):\n",
    "        if i == WINDOW_SIZE or window[i] == '<DUMMY>': \n",
    "            continue\n",
    "        train_data.append((window[WINDOW_SIZE], window[i]))\n",
    "print(train_data[:10])\n",
    "print(word2index['['])\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "       \n",
    "     \n",
    "def prepare_sequence(seq, word2index):\n",
    "    idxs = list(   map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq)         ) \n",
    "    return Variable(LongTensor(idxs))\n",
    "\n",
    "\n",
    "def prepare_word(word, word2index):\n",
    "    return Variable(LongTensor([word2index[word]])   if word2index.get(word) is not None else LongTensor([word2index[\"<UNK>\"]]))   \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_p=  tensor([ 475])        y_p= tensor([ 288])\nX_p=  tensor([[ 475]])      y_p= tensor([[ 288]]) \n\nX_p=  tensor([ 475])        y_p= tensor([ 441])\nX_p=  tensor([[ 475]])      y_p= tensor([[ 441]]) \n\n"
     ]
    }
   ],
   "source": [
    "X_p = []\n",
    "y_p = []\n",
    "\n",
    "for tr in train_data[:2]:\n",
    "    print(\"X_p= \",prepare_word(tr[0],word2index),\"       y_p=\",prepare_word(tr[1],word2index))\n",
    "    print(\"X_p= \",prepare_word(tr[0],word2index).view(1,-1),\"     y_p=\",prepare_word(tr[1],word2index).view(1,-1),'\\n')\n",
    "    \n",
    "#view相当于reshape               tensor的数据个数是6个，如果view（1，-1），我们就可以根据tensor的元素个数推断出-1代表6，维度为1*6\n",
    "for tr in train_data:\n",
    "    X_p.append(prepare_word(tr[0], word2index).view(1, -1))\n",
    "    y_p.append(prepare_word(tr[1], word2index).view(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1])\n[tensor([[ 475]]), tensor([[ 475]]), tensor([[ 475]]), tensor([[ 288]])]\n[tensor([[ 288]]), tensor([[ 441]]), tensor([[ 481]]), tensor([[ 475]])]\n[(tensor([[ 475]]), tensor([[ 288]])), (tensor([[ 475]]), tensor([[ 441]])), (tensor([[ 475]]), tensor([[ 481]])), (tensor([[ 288]]), tensor([[ 475]]))]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"'\\n[\\t(tensor([[ 100]]), tensor([[ 345]])),           ========>('[', 'moby')\\n\\t(tensor([[ 100]]), tensor([[ 245]])),           ========> ('[', 'dick')\\n\\t(tensor([[ 100]]), tensor([[ 571]])),           ========>  ('[', 'by'),\\n\\t(tensor([[ 345]]), tensor([[ 100]]))            ========>  ('moby', '[')\\n]\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_p[:4][0].size())\n",
    "print(X_p[:4])\n",
    "print(y_p[:4])\n",
    "train_data = list(zip(X_p, y_p))\n",
    "print(train_data[:4])\n",
    "''''\n",
    "[\t(tensor([[ 100]]), tensor([[ 345]])),           ========>('[', 'moby')\n",
    "\t(tensor([[ 100]]), tensor([[ 245]])),           ========> ('[', 'dick')\n",
    "\t(tensor([[ 100]]), tensor([[ 571]])),           ========>  ('[', 'by'),\n",
    "\t(tensor([[ 345]]), tensor([[ 100]]))            ========>  ('moby', '[')\n",
    "]\n",
    "'''\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7606"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "583\n"
     ]
    }
   ],
   "source": [
    "print(len(word2index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0, mean_loss : 1.93\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 10, mean_loss : 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 20, mean_loss : 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 30, mean_loss : 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 40, mean_loss : 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 50, mean_loss : 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 60, mean_loss : 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 70, mean_loss : 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 80, mean_loss : 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 90, mean_loss : 0.00\n"
     ]
    }
   ],
   "source": [
    "def getBatch(batch_size, train_data):\n",
    "    random.shuffle(train_data)\n",
    "    sindex = 0\n",
    "    eindex = batch_size\n",
    "    while eindex < len(train_data):\n",
    "        batch = train_data[sindex: eindex]\n",
    "        temp = eindex\n",
    "        eindex = eindex + batch_size\n",
    "        sindex = temp\n",
    "        yield batch\n",
    "    \n",
    "    if eindex >= len(train_data):\n",
    "        batch = train_data[sindex:]\n",
    "        yield batch\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "class Skipgram(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, projection_dim):\n",
    "        super(Skipgram,self).__init__()\n",
    "        self.embedding_v = nn.Embedding(vocab_size, projection_dim)   # D x V\n",
    "        self.embedding_u = nn.Embedding(vocab_size, projection_dim)\n",
    "\n",
    "        self.embedding_v.weight.data.uniform_(-1, 1) # init\n",
    "        self.embedding_u.weight.data.uniform_(0, 0) # init\n",
    "        #self.out = nn.Linear(projection_dim,vocab_size)\n",
    "    def forward(self, center_words,target_words, outer_words):\n",
    "        center_embeds = self.embedding_v(center_words) # B x 1 x D            V_c = (d*v) * (v * b) = (d *b)\n",
    "        target_embeds = self.embedding_u(target_words) # B x 1 x D            U_o = (d*v) * (v * b) = (d *b)\n",
    "        outer_embeds = self.embedding_u(outer_words) # B x V x D              U_w = (d*v) * (v * b) = (d * b)\n",
    "        \n",
    "        scores = target_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)          # (V_c.T * U_o) =  (b*d) *(d*b) = (b*b)\n",
    "        norm_scores = outer_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)      # (V_c.T * V_w)=   (b*d) * (d*b) =(b*b)\n",
    "        \n",
    "        nll = -torch.mean(torch.log(torch.exp(scores)/torch.sum(torch.exp(norm_scores), 1).unsqueeze(1))) # log-softmax\n",
    "        \n",
    "        return nll # negative log likelihood\n",
    "    \n",
    "    def prediction(self, inputs):\n",
    "        embeds = self.embedding_v(inputs)\n",
    "        \n",
    "        return embeds \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "EMBEDDING_SIZE = 30\n",
    "BATCH_SIZE = 256\n",
    "EPOCH = 100\n",
    "\n",
    "\n",
    "model = Skipgram(len(word2index), EMBEDDING_SIZE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "losses = []\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    # a=getBatch(BATCH_SIZE,train_data)\n",
    "    # print(type(a) )  #<class 'generator'>\n",
    "    for i, batch in enumerate(getBatch(BATCH_SIZE, train_data)):\n",
    "        # if i==2:\n",
    "        #     print(\"\\n1\",batch)    #batch是元组组成的数组\n",
    "            # print(\"\\n2\",*batch)  #  *batch是元组\n",
    "        '''\n",
    "           [  (tensor([[ 341]]), tensor([[ 26]])), \n",
    "                (tensor([[ 215]]), tensor([[ 429]])), \n",
    "                (tensor([[ 361]]), tensor([[ 508]]))] \n",
    "                \n",
    "            (tensor([[ 341]]), tensor([[ 26]])) \n",
    "            (tensor([[ 215]]), tensor([[ 429]])) \n",
    "            (tensor([[ 361]]), tensor([[ 508]]))\n",
    "         '''\n",
    "        inputs, targets = zip(*batch) \n",
    "        # if i==2:\n",
    "        #     print(inputs)\n",
    "        #         (tensor([[ 47]]),    tensor([[ 0]]),   tensor([[ 163]]) \n",
    "        inputs = torch.cat(inputs) # B x 1\n",
    "        # if i==2:\n",
    "        #     print('\\n',inputs)\n",
    "       #         tensor ( [  [ 47], [0], [ 163]  ] ) \n",
    "        targets = torch.cat(targets) # B x 1\n",
    "        \n",
    "        \n",
    "        vocabs = prepare_sequence(list(vocab), word2index).expand(inputs.size(0), len(vocab))  # B x V\n",
    "        \n",
    "        \n",
    "        \n",
    "        model.zero_grad()\n",
    "        loss = model(inputs, targets, vocabs)\n",
    "        #反向传播\n",
    "        loss.backward()\n",
    "        #优化梯度   w = w + lr * grad\n",
    "        optimizer.step()\n",
    "    \n",
    "   \n",
    "        losses.append(loss.data.tolist())\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch : %d, mean_loss : %.02f\" % (epoch,np.mean(losses)))\n",
    "        losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plainly\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['these', 0.5557811856269836],\n ['splintered', 0.5465330481529236],\n ['monstrous', 0.5317400097846985],\n ['down', 0.5246033668518066],\n ['random', 0.46905726194381714],\n ['ruin', 0.4616011381149292],\n ['hwal', 0.4541151821613312],\n ['learned', 0.4469209909439087],\n ['dart', 0.4407564699649811],\n ['was', 0.42691487073898315],\n ['piercing', 0.4191505014896393],\n ['lucian', 0.41689443588256836],\n ['were', 0.3968399167060852],\n ['one', 0.38981321454048157],\n ['to', 0.38828083872795105],\n ['deliver', 0.38629209995269775],\n ['mockingly', 0.38288551568984985],\n ['consumptive', 0.3825888931751251],\n ['god', 0.35731345415115356],\n ['rolling', 0.35263940691947937],\n ['sit', 0.34989187121391296],\n ['mast', 0.34026435017585754],\n ['burrower', 0.33982089161872864],\n ['whatever', 0.3328428566455841],\n ['parmacetti', 0.33131229877471924],\n ['pampered', 0.32948189973831177],\n ['quid', 0.3240537941455841],\n ['but', 0.3191649615764618],\n ['reminded', 0.3175491392612457],\n ['bruise', 0.2970965504646301],\n ['devilish', 0.2948283851146698],\n ['waves', 0.294216126203537],\n ['pekee', 0.28949788212776184],\n ['lord', 0.28838616609573364],\n ['calm', 0.2804127037525177],\n ['least', 0.27462416887283325],\n ['tears', 0.27164870500564575],\n ['wallen', 0.2700207233428955],\n ['belongest', 0.2664943337440491],\n [']', 0.26309433579444885],\n ['six', 0.26035943627357483],\n ['dinting', 0.2598530054092407],\n ['therein', 0.255516916513443],\n ['altogether', 0.2510092854499817],\n ['annals', 0.24923229217529297],\n ['mortality', 0.2479371875524521],\n ['feel', 0.24636003375053406],\n ['threadbare', 0.24445077776908875],\n [':', 0.24409347772598267],\n ['letter', 0.24287191033363342]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def word_similarity(target, vocab):\n",
    "    target_V = model.prediction(prepare_word(target, word2index))\n",
    "    similarities = []\n",
    "    for i in range(len(vocab)):\n",
    "        if vocab[i] == target: continue\n",
    "        vector = model.prediction(prepare_word(list(vocab)[i], word2index))\n",
    "        cosine_sim = F.cosine_similarity(target_V, vector).data.tolist()[0] \n",
    "        similarities.append([vocab[i], cosine_sim])\n",
    "    return sorted(similarities, key=lambda x: x[1], reverse=True)[:50]   # sort by similarity\n",
    "\n",
    "test = random.choice(list(vocab))\n",
    "print(test)\n",
    "word_similarity(test, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
