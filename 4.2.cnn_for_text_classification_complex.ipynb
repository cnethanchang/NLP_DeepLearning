{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n这个项目例句:    'DESC:manner How did serfdom develop in and then leave Russia ?' \\n转化为：   X= ['manner',' How ',' did ',' serfdom','  develop ',' in ',' and ',' then','  leave','  Russia','  ?']     y= 'desc'\\n转化为：   X= [234,14,1,41124,1414,1414]     y=5\\nX转化为   300维度的 wordvector\\nmax_len 表示每个batch X最长的长度\\n\\n(d,v) * (v,max_len,b) =(d, max_len ,b)\\n\\nK=(3,4,5)  K表示strid     C_f 表示C_f个filter                       #三次不同filter size的卷积操作，然后用maxpool得到相同的size。然后concate\\n(d,max_len,b) 卷积操作_*   C_f个(d, K) =  (max_len-K+1,b,C_f)     K为3/4/5\\n然后三个(max_len-K+1,b,C_f)对于不同的max_len-K+1分别max_pool，得到三个(b,C_f)  \\n三个(b,C_f)  concate得到 (b,3*C_f)  \\n(out_size,3*C_f) * (b,3*C_f) = (out_size,b)\\n\\n(out_size,b)  与b个y   softmax  得到结果\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter, OrderedDict\n",
    "import nltk\n",
    "import re\n",
    "from copy import deepcopy\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "random.seed(1024)\n",
    "\n",
    "FloatTensor = torch.FloatTensor\n",
    "LongTensor = torch.LongTensor\n",
    "ByteTensor = torch.ByteTensor\n",
    "\n",
    "\n",
    "'''\n",
    "这个项目例句:    'DESC:manner How did serfdom develop in and then leave Russia ?' \n",
    "转化为：   X= ['manner',' How ',' did ',' serfdom','  develop ',' in ',' and ',' then','  leave','  Russia','  ?']     y= 'desc'\n",
    "转化为：   X= [234,14,1,41124,1414,1414]     y=5\n",
    "X转化为   300维度的 wordvector\n",
    "max_len 表示每个batch X最长的长度\n",
    "\n",
    "(d,v) * (v,max_len,b) =(d, max_len ,b)\n",
    "\n",
    "K=(3,4,5)  K表示strid     C_f 表示C_f个filter                       #三次不同filter size的卷积操作，然后用maxpool得到相同的size。然后concate\n",
    "(d,max_len,b) 卷积操作_*   C_f个(d, K) =  (max_len-K+1,b,C_f)     K为3/4/5\n",
    "然后三个(max_len-K+1,b,C_f)对于不同的max_len-K+1分别max_pool，得到三个(b,C_f)  \n",
    "三个(b,C_f)  concate得到 (b,3*C_f)  \n",
    "(out_size,3*C_f) * (b,3*C_f) = (out_size,b)\n",
    "\n",
    "(out_size,b)  与b个y   softmax  得到结果\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBatch(batch_size, train_data):\n",
    "    random.shuffle(train_data)\n",
    "    sindex = 0\n",
    "    eindex = batch_size\n",
    "    while eindex < len(train_data):\n",
    "        batch = train_data[sindex: eindex]\n",
    "        temp = eindex\n",
    "        eindex = eindex + batch_size\n",
    "        sindex = temp\n",
    "        yield batch\n",
    "    \n",
    "    if eindex >= len(train_data):\n",
    "        batch = train_data[sindex:]\n",
    "        yield batch\n",
    "        \n",
    "def pad_to_batch(batch):\n",
    "    x,y = zip(*batch)\n",
    "    max_x = max([s.size(1) for s in x])\n",
    "    # print(max_x)\n",
    "    x_p = []\n",
    "    for i in range(len(batch)):\n",
    "        if x[i].size(1) < max_x:\n",
    "            x_p.append(torch.cat([x[i], Variable(LongTensor([word2index['<PAD>']] * (max_x - x[i].size(1)))).view(1, -1)], 1))\n",
    "        else:\n",
    "            x_p.append(x[i])\n",
    "    return torch.cat(x_p), torch.cat(y).view(-1)\n",
    "\n",
    "\n",
    "def prepare_sequence(seq, to_index):\n",
    "    idxs = list(map(lambda w: to_index[w] if to_index.get(w) is not None else to_index[\"<UNK>\"], seq))\n",
    "    return Variable(LongTensor(idxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ind Name 11 famous martyrs . ========> ['ind', 'Name', '##', 'famous', 'martyrs', '.']\ndesc What 's the Olympic motto ? ========> ['desc', 'What', \"'s\", 'the', 'Olympic', 'motto', '?']\ndesc What is the origin of the name ` Scarlett ' ? ========> ['desc', 'What', 'is', 'the', 'origin', 'of', 'the', 'name', '`', 'Scarlett', \"'\", '?']\n"
     ]
    }
   ],
   "source": [
    "data = open(r'C:\\workspace\\python-work\\nlp\\NLP-cs224n\\NLP-cs224n_ec\\notebooks\\dataset\\cnn_classification\\train_5500.labe.txt', 'r', encoding='latin-1').readlines()\n",
    "#data[0]='DESC:manner How did serfdom develop in and then leave Russia ?' \n",
    "data = [[d.split(':')[1][:-1], d.split(':')[0]] for d in data]\n",
    "#data[0] = ['manner How did serfdom develop in and then leave Russia ?',  'DESC']\n",
    "\n",
    "X, y = list(zip(*data))\n",
    "# print(X,'\\n')\n",
    "#  ('manner How did serfdom develop in and then leave Russia ?', \n",
    "#  'cremat What films featured the character Popeye Doyle ?'.......)\n",
    "\n",
    "X = list(X)\n",
    "# print(X)\n",
    "#  ['manner How did serfdom develop in and then leave Russia ?', \n",
    "#  'cremat What films featured the character Popeye Doyle ?'.......]\n",
    "\n",
    "\n",
    "for i, x in enumerate(X):\n",
    "    X[i] = re.sub('\\d', '#', x).split()\n",
    "    if i in range(22,25):\n",
    "        print(x,'========>',X[i])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9117\nlen(y)= 6   set(y)= {'ABBR', 'DESC', 'NUM', 'ENTY', 'LOC', 'HUM'}\n{'techmeth', 'currency', 'food', 'product', 'veh', 'volsize', 'body', 'speed', 'letter', 'ord', 'animal', 'state', 'lang', 'color', 'mount', 'manner', 'exp', 'religion', 'date', 'symbol', 'sport', 'instru', 'dismed', 'title', 'substance', 'temp', 'other', 'abb', 'def', 'plant', 'dist', 'weight', 'gr', 'reason', 'termeq', 'city', 'code', 'event', 'count', 'country', 'cremat', 'ind', 'word', 'desc', 'period', 'money', 'perc'}\n"
     ]
    }
   ],
   "source": [
    "vocab = list(set(flatten(X)))\n",
    "print(len(vocab))\n",
    "#X中单词的种类   9117\n",
    "\n",
    "print('len(y)=',len(set(y)),'  set(y)=',set(y))\n",
    "#y的种类\n",
    "# len(y)= 6   set(y)= {'ABBR', 'LOC', 'NUM', 'HUM', 'ENTY', 'DESC'}\n",
    "\n",
    "print(  set( [ i[0]    for i in X]))\n",
    "#X数组中的第一个单词\n",
    "#{'ord', 'word', 'product', 'sport', 'body', 'termeq', 'manner', 'volsize', 'code', 'weight', 'cremat', 'desc', 'veh', 'lang', 'food', 'ind', 'city', 'instru', 'count', 'substance', 'gr', 'letter', 'other', 'color', 'country', 'animal', 'state', 'religion', 'dismed', 'exp', 'period', 'currency', 'mount', 'money', 'symbol', 'def', 'plant', 'abb', 'event', 'title', 'speed', 'temp', 'reason', 'perc', 'dist', 'date', 'techmeth'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index={'<PAD>': 0, '<UNK>': 1}\n",
    "\n",
    "for vo in vocab:\n",
    "    if word2index.get(vo) is None:\n",
    "        word2index[vo] = len(word2index)\n",
    "        \n",
    "index2word = {v:k for k, v in word2index.items()}\n",
    "\n",
    "target2index = {}\n",
    "\n",
    "for cl in set(y):\n",
    "    if target2index.get(cl) is None:\n",
    "        target2index[cl] = len(target2index)\n",
    "\n",
    "index2target = {v:k for k, v in target2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_p, y_p = [], []\n",
    "for pair in zip(X,y):\n",
    "      #pair       (['ind', 'Name', '##', 'famous', 'martyrs', '.'],'DESC')\n",
    "    X_p.append(prepare_sequence(pair[0], word2index).view(1, -1))             #append([ 234,14,1,41124,1414,1414   ])\n",
    "    y_p.append(Variable(LongTensor([target2index[pair[1]]])).view(1, -1))\n",
    "    \n",
    "data_p = list(zip(X_p, y_p))       #  [ ([ 234,14,1,41124,1414,1414 ],5)   ,   ( [ 234,14,1,41124,1414,1414 ],5 ),..........]\n",
    "random.shuffle(data_p)\n",
    "\n",
    "train_data = data_p[: int(len(data_p) * 0.9)]\n",
    "test_data = data_p[int(len(data_p) * 0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "#导入wordvect  ，300 dimention，3，000，000个单词\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(r'C:\\workspace\\python-work\\nlp\\NLP-cs224n\\NLP-cs224n_ec\\notebooks\\dataset\\cnn_classification\\GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "len(model.index2word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n(9119, 300)\n6\n"
     ]
    }
   ],
   "source": [
    "pretrained = []\n",
    "#将一维的word转化为300维的wordvector\n",
    "for key in word2index.keys():\n",
    "    try:\n",
    "        pretrained.append(model[word2index[key]])\n",
    "    except:\n",
    "        pretrained.append(np.random.randn(300))\n",
    "        \n",
    "# print(pretrained[0])       #[ 124,24124,24...]  300d\n",
    "pretrained_vectors = np.vstack(pretrained)\n",
    "\n",
    "print(pretrained_vectors[0].shape)  #[ [ 124,24124,24...] ]  1*300d \n",
    "#(300,)\n",
    "print(pretrained_vectors.shape)  #[ [ 124,24124,24...] ]  1*300d \n",
    "\n",
    "print(len(target2index))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model===========> CNNClassifier(\n  (embedding): Embedding(9119, 300)\n  (convs): ModuleList(\n    (0): Conv2d(1, 100, kernel_size=(3, 300), stride=(1, 1))\n    (1): Conv2d(1, 100, kernel_size=(4, 300), stride=(1, 1))\n    (2): Conv2d(1, 100, kernel_size=(5, 300), stride=(1, 1))\n  )\n  (dropout): Dropout(p=0.5)\n  (fc): Linear(in_features=300, out_features=6, bias=True)\n)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/1] mean_loss : 2.22\n"
     ]
    }
   ],
   "source": [
    "class  CNNClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, output_size, kernel_dim=100, kernel_sizes=(3, 4, 5), dropout=0.5):\n",
    "        super(CNNClassifier,self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)      #(v,d)\n",
    "        self.convs = nn.ModuleList(   [nn.Conv2d(1, kernel_dim, (K, embedding_dim))     for K in kernel_sizes]          )\n",
    "\n",
    "        # kernal_size = (K,D) \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(len(kernel_sizes) * kernel_dim, output_size)  #(6,300)\n",
    "    \n",
    "    \n",
    "    def init_weights(self, pretrained_word_vectors, is_static=False):\n",
    "        self.embedding.weight = nn.Parameter(torch.from_numpy(pretrained_word_vectors).float())\n",
    "        if is_static:\n",
    "            self.embedding.weight.requires_grad = False\n",
    "\n",
    "\n",
    "    def forward(self, inputs, is_training=False):\n",
    "        inputs = self.embedding(inputs).unsqueeze(1)     # (d,v) * (v,max_length,B)  = (d,1,max_length,b)\n",
    "        #print('1size=',inputs.size())                         # 1size= torch.Size([50, 1, 21, 300] )            (b,1,max_len,d)\n",
    "        #下行对于每一个   (d,1,max_len,b) 卷积_*  （d,K) =  (max_len-K+1,b) \n",
    "        inputs = [F.relu(conv(inputs)).squeeze(3)    for conv in self.convs]   #[(b,C_filter,max_len-K+1 )*3 ]     #max_len=18,k=(3,4,5),C_filter=100,b=50\n",
    "        # print(\"inputs[0].size=\",inputs[0].size())      #inputs[0].size= torch.Size([50, 100, 16])\n",
    "        # print(\"inputs[1].size=\",inputs[1].size())      #inputs[1].size= torch.Size([50, 100, 15])\n",
    "        # print(\"inputs[2].size=\",inputs[2].size())      #inputs[2].size= torch.Size([50, 100, 14]) \n",
    "        inputs = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in inputs]       #[(b,C_filter)*3]\n",
    "        # print(\"inputs[0].size=\",inputs[0].size())      #inputs[0].size= torch.Size([50, 100])\n",
    "        # print(\"inputs[1].size=\",inputs[1].size())      # inputs[1].size= torch.Size([50, 100])\n",
    "        # print(\"inputs[2].size=\",inputs[2].size())        # inputs[2].size= torch.Size([50, 100])\n",
    "\n",
    "        concated = torch.cat(inputs, 1)\n",
    "        # print(concated.size())   #torch.Size([50, 300]) =  torch.Size([50, 100*3])\n",
    "\n",
    "        if is_training:\n",
    "            concated = self.dropout(concated) # (N,len(Ks)*Co)\n",
    "        out = self.fc(concated)     #  (6,300)*(300,50)  = (6,50) = (out_size,batch)\n",
    "        # print(out.size())   #torch.Size([50, 6])\n",
    "        return F.log_softmax(out,1)\n",
    "    \n",
    "EPOCH = 1\n",
    "BATCH_SIZE = 50\n",
    "KERNEL_SIZES = [3,4,5]\n",
    "KERNEL_DIM = 100\n",
    "LR = 0.001\n",
    "\n",
    "model = CNNClassifier(len(word2index), 300, len(target2index), KERNEL_DIM, KERNEL_SIZES)    #  (9117,300,6,[3,4,5],100)\n",
    "print(\"model===========>\",model)\n",
    "\n",
    "'''\n",
    "CNNClassifier(\n",
    "  (embedding): Embedding(9119, 300)\n",
    "  (convs): ModuleList(\n",
    "    (0): Conv2d(1, 100, kernel_size=(3, 300), stride=(1, 1))\n",
    "    (1): Conv2d(1, 100, kernel_size=(4, 300), stride=(1, 1))\n",
    "    (2): Conv2d(1, 100, kernel_size=(5, 300), stride=(1, 1))\n",
    "  )\n",
    "  (dropout): Dropout(p=0.5)\n",
    "  (fc): Linear(in_features=300, out_features=6, bias=True)\n",
    ")\n",
    "'''\n",
    "model.init_weights(pretrained_vectors) # initialize embedding matrix using pretrained vectors  (9119,300)\n",
    "\n",
    "\n",
    "    \n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    losses = []\n",
    "    for i,batch in enumerate(getBatch(BATCH_SIZE, train_data)):\n",
    "        inputs,targets = pad_to_batch(batch)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        preds = model(inputs, True)  #input  (max_length,B,v)  \n",
    "        # print(preds.size())    #log_softmax之后的   #(6,50) = (out_size,batch)\n",
    "        loss = loss_function(preds, targets)\n",
    "        losses.append(loss.data.tolist())\n",
    "        loss.backward()\n",
    "        \n",
    "        #for param in model.parameters():\n",
    "        #    param.grad.data.clamp_(-3, 3)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(\"[%d/%d] mean_loss : %0.2f\" %(epoch, EPOCH, np.mean(losses)))\n",
    "            losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 tensor([ 3])\n2 3\n3\n"
     ]
    }
   ],
   "source": [
    "accuracy = 0\n",
    "wrongn=0\n",
    "#test_data =   [ ([ 234,14,1,41124,1414,1414 ],5)   ,   ( [ 234,14,1,41124,1414,1414 ],5 ),..........]\n",
    "for i,test in enumerate(test_data):\n",
    "    if(i==0):\n",
    "        try:\n",
    "            # print(i,test)f\n",
    "            pred = model(test[0]).max(1)[1]\n",
    "            print(1,pred)\n",
    "            pred = pred.data.tolist()[0]\n",
    "            print(2,pred)\n",
    "            target = test[1].data.tolist()[0][0]\n",
    "            print(target)\n",
    "            if pred == target:\n",
    "                accuracy += 1\n",
    "            else:\n",
    "                print(i)\n",
    "        except:\n",
    "            wrongn+=1\n",
    "            print(wrong,\"=>\",i)\n",
    "    # print('accuracy=',accuracy/len(test_data) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 tensor([[-3.2156, -4.8130, -2.5072, -0.4250, -1.5331, -7.3604]])\n2 [-3.2156078815460205, -4.812989234924316, -2.507150173187256, -0.42504698038101196, -1.5330772399902344, -7.360416889190674]\n3 3\n0\n"
     ]
    }
   ],
   "source": [
    "accuracy = 0\n",
    "wrongn=0\n",
    "#test_data =   [ ([ 234,14,1,41124,1414,1414 ],5)   ,   ( [ 234,14,1,41124,1414,1414 ],5 ),..........]\n",
    "for i,test in enumerate(test_data):\n",
    "    if(i==0 ):\n",
    "        try:\n",
    "            # print(i,test)f\n",
    "            pred = model(test[0])\n",
    "            print(1,pred)\n",
    "            pred = pred.data.tolist()[0]\n",
    "            print(2,pred)\n",
    "            target = test[1].data.tolist()[0][0]\n",
    "            print(3,target)\n",
    "            if pred == target:\n",
    "                accuracy += 1\n",
    "            else:\n",
    "                print(i)\n",
    "        except:\n",
    "            wrongn+=1\n",
    "            print(wrong,\"=>\",i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
