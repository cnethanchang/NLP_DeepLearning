{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n另外一个cnntext项目例句:    'DESC:manner How did serfdom develop in and then leave Russia ?' \\n转化为：   X= ['manner',' How ',' did ',' serfdom','  develop ',' in ',' and ',' then','  leave','  Russia','  ?']     y= 'desc'\\n转化为：   X= [234,14,1,41124,1414,1414]     y=5\\nX转化为   300维度的 wordvector\\nmax_len 表示每个batch X最长的长度\\n\\n(d,v) * (v,max_len,b) =(d, max_len ,b)\\n\\nK=(3,4,5)  K表示strid     C_f 表示C_f个filter         #三次不同filter size的卷积操作得到，然后用maxpool得到相同的size=(b,C_f,1,1)。然后concate\\n(d,max_len,b) 卷积操作_*   C_f个(d, K) =  (max_len-K+1,b,C_f)     K为3/4/5\\n然后三个(max_len-K+1,b,C_f)对于不同的max_len-K+1分别max_pool，得到三个(b,C_f)  \\n三个(b,C_f)  concate得到 (b,3*C_f)  \\n(out_size,3*C_f) * (b,3*C_f) = (out_size,b)\\n\\n(out_size,b)  与b个y   softmax  得到结果\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "另外一个cnntext项目例句:    'DESC:manner How did serfdom develop in and then leave Russia ?' \n",
    "转化为：   X= ['manner',' How ',' did ',' serfdom','  develop ',' in ',' and ',' then','  leave','  Russia','  ?']     y= 'desc'\n",
    "转化为：   X= [234,14,1,41124,1414,1414]     y=5\n",
    "X转化为   300维度的 wordvector\n",
    "max_len 表示每个batch X最长的长度\n",
    "\n",
    "(d,v) * (v,max_len,b) =(d, max_len ,b)\n",
    "\n",
    "K=(3,4,5)  K表示strid     C_f 表示C_f个filter         #三次不同filter size的卷积操作得到，然后用maxpool得到相同的size=(b,C_f,1,1)。然后concate\n",
    "(d,max_len,b) 卷积操作_*   C_f个(d, K) =  (max_len-K+1,b,C_f)     K为3/4/5\n",
    "然后三个(max_len-K+1,b,C_f)对于不同的max_len-K+1分别max_pool，得到三个(b,C_f)  \n",
    "三个(b,C_f)  concate得到 (b,3*C_f)  \n",
    "(out_size,3*C_f) * (b,3*C_f) = (out_size,b)\n",
    "\n",
    "(out_size,b)  与b个y   softmax  得到结果\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " word_dict= {'for': 0, 'is': 1, 'sorry': 2, 'loves': 3, 'likes': 4, 'that': 5, 'me': 6, 'love': 7, 'he': 8, 'this': 9, 'awful': 10, 'baseball': 11, 'you': 12, 'hate': 13, 'i': 14, 'she': 15}\nvocab_size= 16\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "\n",
    "\n",
    "\n",
    "# 3 words sentences (=sequence_length is 3)\n",
    "sentences = [\"i love you\", \"he loves me\", \"she likes baseball\", \"i hate you\", \"sorry for that\", \"this is awful\"]\n",
    "labels = [1, 1, 1, 0, 0, 0]  # 1 is good, 0 is not good.\n",
    "\n",
    "word_list = \" \".join(sentences).split()\n",
    "word_list = list(set(word_list))\n",
    "word_dict = {w: i for i, w in enumerate(word_list)}\n",
    "vocab_size = len(word_dict)   #v=16\n",
    "\n",
    "\n",
    "\n",
    "print(' word_dict=',word_dict)\n",
    "print('vocab_size=',vocab_size)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input= torch.Size([6, 3])\noutput= torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "# Text-CNN Parameter\n",
    "embedding_size = 2                #d=2\n",
    "sequence_length = 3               #m_length=m=3\n",
    "num_classes = 2                   # 0 or 1   => out_size     \n",
    "filter_sizes = [2, 2, 2]          # n-gram window  =>\n",
    "num_filters = 3                   # =>f\n",
    "\n",
    "\n",
    "inputs = []\n",
    "targets = []\n",
    "for sen in sentences:\n",
    "    inputs.append([word_dict[n] for n in sen.split()])\n",
    "\n",
    "\n",
    "for out in labels:\n",
    "    targets.append(out) # To using Torch Softmax Loss function\n",
    "\n",
    "input_batch = Variable(torch.LongTensor(inputs))  #(b,m)\n",
    "target_batch = Variable(torch.LongTensor(targets))\n",
    "\n",
    "print(\"input=\",input_batch.size()) \n",
    "print(\"output=\",target_batch.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextCNN, self).__init__()\n",
    "\n",
    "        self.num_filters_total = num_filters * len(filter_sizes)       #3*out_channel\n",
    "        self.W = nn.Parameter(torch.empty(vocab_size, embedding_size).uniform_(-1, 1)).type(dtype)      #(v,d)\n",
    "        self.Weight = nn.Parameter(torch.empty(self.num_filters_total, num_classes).uniform_(-1, 1)).type(dtype) #(out_size,3*out_channel)\n",
    "        self.Bias = nn.Parameter(0.1 * torch.ones([num_classes])).type(dtype)\n",
    "\n",
    "    def forward(self, X):                      #(b,m)\n",
    "        embedded_chars = self.W[X]             # (v,d) * (b,m)=(b,m,d)=(6,3,2)      embeddign\n",
    "        embedded_chars = embedded_chars.unsqueeze(1)       #(b,m,d)===>(b,1,m,d)    add input_channel=1     [b, input_channel(=1), m, d]\n",
    "\n",
    "        pooled_outputs = []\n",
    "        for filter_size in filter_sizes:\n",
    "                    # conv2d( input_channel(=1), output_channel(=3), (filter_height(=2), filter_width(=d) ), bias_option )\n",
    "            conv = nn.Conv2d(1, num_filters, (filter_size, embedding_size), bias=True)(embedded_chars)        #embedded_chars:(b,1,m,d) \n",
    "            #conv:(b,output_channel,m-filter_height+1,d-filter_width+1)=(6,3,3-2+1,d-d+1)=(6,3,2,1)\n",
    "                    \n",
    "                    \n",
    "            h = F.relu(conv)\n",
    "            # mp : ((filter_height, filter_width))\n",
    "            mp = nn.MaxPool2d((sequence_length - filter_size + 1, 1))\n",
    "            mp=  mp(h) #mp:   (b,ouput_channel,2,1)=======>maxpool====>(b,output_channle,1,1)=(6,3,1,1)\n",
    "\n",
    "            pooled = mp.permute(0, 3, 2, 1)    #pooled: (b,output_channel,1,1)====>(b,1,1,output_channel)=(6,1,1,3)\n",
    "            pooled_outputs.append(pooled)\n",
    "\n",
    "        h_pool = torch.cat(pooled_outputs, len(filter_sizes)) #(b,1,1,output_channel*3)=(6,1,1,9)\n",
    "        h_pool_flat = torch.reshape(h_pool, [-1, self.num_filters_total])   #(b,3*out_channel)=(6,9)\n",
    "\n",
    "        model = torch.mm(h_pool_flat, self.Weight) + self.Bias     #(out_size,3*out_channel)* (b,3*out_channel)=(b,out_size)\n",
    "        return model\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 2])\ntensor([[-0.2810,  1.1076],\n        [-0.5391,  0.1760],\n        [-0.3084,  0.6615],\n        [-0.2210,  0.6378],\n        [-0.3741,  1.1915],\n        [-0.2388,  1.0404]])\ntorch.Size([6])\ntensor([ 1,  1,  1,  0,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "model = TextCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Training\n",
    "for epoch in range(1):\n",
    "# for epoch in range(5000):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(input_batch) #(b,m)\n",
    "    print(output.size())\n",
    "    print(output)\n",
    "    print(target_batch.size())\n",
    "    print(target_batch)\n",
    "    # output : (b,out_size)        target_batch :(b) (LongTensor, not one-hot)\n",
    "    loss = criterion(output, target_batch)\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9616,  1.3497]])\n(tensor([[ 1.6426]]), tensor([[ 1]]))\ntensor([[ 1]])\n' me love you ' is Good Mean\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "test_text = 'me love you'\n",
    "tests = [[word_dict[n] for n in test_text.split()]]\n",
    "test_batch = Variable(torch.LongTensor(tests))\n",
    "\n",
    "\n",
    "# Predict\n",
    "predict = model(test_batch)\n",
    "print(predict)\n",
    "\n",
    "\n",
    "\n",
    "# Predict\n",
    "predict = model(test_batch).data.max(1, keepdim=True)\n",
    "print(predict)\n",
    "\n",
    "# Predict\n",
    "predict = model(test_batch).data.max(1, keepdim=True)[1]\n",
    "print(predict)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if predict[0][0] == 0:\n",
    "    print(\"'\",test_text,\"'\",\"is Bad Mean\")\n",
    "else:\n",
    "    print(\"'\",test_text,\"'\",\"is Good Mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0265,  1.5113]])\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "test_text = 'me love you'\n",
    "tests = [[word_dict[n] for n in test_text.split()]]\n",
    "test_batch = Variable(torch.LongTensor(tests))\n",
    "\n",
    "# Predict\n",
    "predict = model(test_batch).data\n",
    "print(predict)\n",
    "# if predict[0][0] == 0:\n",
    "#     print(\"'\",test_text,\"'\",\"is Bad Mean\")\n",
    "# else:\n",
    "#     print(\"'\",test_text,\"'\",\"is Good Mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
