{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert\n",
    "'''\n",
    "原始语句：\n",
    "    'Hello, how are you? I am Romeo.\\n'\n",
    "    'Hello, Romeo My name is Juliet. Nice to meet you.\\n'\n",
    "    'Nice meet you too. How are you today?\\n'\n",
    "    'Great. My baseball team won the competition.\\n'\n",
    "    'Oh Congratulations, Juliet\\n'\n",
    "    'Thanks you Romeo'\n",
    "  ===========>\n",
    "    [5, 13, 25, 20, 22, 6, 11], \n",
    "    [5, 11, 24, 21, 9, 15, 18, 16, 12, 20], \n",
    "    [18, 12, 20, 19, 13, 25, 20, 14], \n",
    "    [8, 24, 4, 17, 27, 23, 28], \n",
    "    [26, 10, 15], \n",
    "    [7, 20, 11]\n",
    "============>(batch 操作）：每次随机抽取两个句子，一开始大概率是NotNext:False ,直到抽取到一半是IsNext时结束   \n",
    "            比如抽到4,0\n",
    "                  token_embeddign:           [1,       index_of_a,          2,       index_of_b ,                    2]\n",
    "                  token_embeddign:           [1,       26, 10, 15,          2,       5, 13, 25, 20, 22, 6, 11,       2]\n",
    "                  随机masked百分之15,随机抽取cand_maked_pos中的n_pred=2前两个位置作为mask language model替换       masked_pos=[5,10]    masked_tokens=[5,6]\n",
    "                                             [1,       26, 10, 15,          2,       3, 13, 25, 20, 22, 3, 11,       2]\n",
    "                                                                                     5--->3             6---->3\n",
    "                  padding:\n",
    "                    #input_ids     : [1, 26, 10, 15, 2, 3, 13, 25, 20, 22, 3, 11, 2,   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "                    #segment_ids   : [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "                    #masked_tokens : [5, 6, 0, 0, 0]\n",
    "                    #masked_pos    : [5,10, 0, 0, 0]                                                               \n",
    "                    \n",
    "                            #input_ids:      token_embedding,   some token has been substituted\n",
    "                            #segment_ids:    segment_embedding\n",
    "                            # masked_tokens: the token which has been substituded                #作为target，Y\n",
    "                            # masked_pos:    the token position which has been substituded\n",
    "                            # isNext:        the sentence_a and sentence_b isNext or NotNext       #作为target，Y                                                                 \n",
    "    return batch    #batch[i]= [input_ids, segment_ids, masked_tokens, masked_pos, True]\n",
    "    \n",
    "    embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)         (b,m,d)\n",
    "    \n",
    "    \n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import re\n",
    "from random import *\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[23, 5, 20, 6, 26, 16, 21], [23, 21, 9, 18, 7, 13, 28, 24, 10, 6], [28, 10, 6, 27, 5, 20, 6, 14], [25, 9, 11, 22, 12, 15, 19], [8, 4, 13], [17, 6, 21]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n[5, 13, 25, 20, 22, 6, 11], \\n[5, 11, 24, 21, 9, 15, 18, 16, 12, 20], \\n[18, 12, 20, 19, 13, 25, 20, 14], \\n[8, 24, 4, 17, 27, 23, 28], \\n[26, 10, 15], \\n[7, 20, 11]\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = (\n",
    "    'Hello, how are you? I am Romeo.\\n'\n",
    "    'Hello, Romeo My name is Juliet. Nice to meet you.\\n'\n",
    "    'Nice meet you too. How are you today?\\n'\n",
    "    'Great. My baseball team won the competition.\\n'\n",
    "    'Oh Congratulations, Juliet\\n'\n",
    "    'Thanks you Romeo'\n",
    ")\n",
    "\n",
    "\n",
    "sentences = re.sub(\"[.,!?\\\\-]\", '', text.lower()).split(\"\\n\")      # filter '.', ',', '?', '!'\n",
    "# print(sentences)\n",
    "'''\n",
    "[ 'hello how are you i am romeo', \n",
    "'hello romeo my name is juliet nice to meet you', \n",
    "'nice meet you too how are you today', \n",
    "'great my baseball team won the competition', \n",
    "'oh congratulations juliet', \n",
    "'thanks you romeo'\n",
    "]\n",
    "'''\n",
    "word_list = list(set(\" \".join(sentences).split()))\n",
    "# print(len(word_list))   #25\n",
    "# print(word_list)\n",
    "# ['too', 'to', 'romeo', 'congratulations', 'are', 'won', 'am', 'i', 'you', 'nice', 'how', 'my', 'great', 'name', 'meet', 'is', 'thanks', 'team', 'the', 'competition', 'oh', 'juliet', 'hello', 'today', 'baseball']\n",
    "\n",
    "word2index = {'[PAD]' : 0, '[CLS]' : 1, '[SEP]' : 2, '[MASK]' : 3}\n",
    "for i, w in enumerate(word_list):\n",
    "    word2index[w] = i + 4\n",
    "index2word = {i: w for i, w in enumerate(word2index)}\n",
    "vocab_size = len(index2word)   #29\n",
    "\n",
    "\n",
    "token_list = list()\n",
    "for sentence in sentences:\n",
    "    tmp=[word2index[i] for i in sentence.split()]\n",
    "    # print(tmp)\n",
    "    token_list.append(tmp)\n",
    "\n",
    "print(token_list)\n",
    "'''\n",
    "[5, 13, 25, 20, 22, 6, 11], \n",
    "[5, 11, 24, 21, 9, 15, 18, 16, 12, 20], \n",
    "[18, 12, 20, 19, 13, 25, 20, 14], \n",
    "[8, 24, 4, 17, 27, 23, 28], \n",
    "[26, 10, 15], \n",
    "[7, 20, 11]\n",
    "'''\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index----------> 1\nFalse  NotNext 0\nindex----------> 2\nFalse  NotNext 1\nindex----------> 3\nFalse  NotNext 2\nindex----------> 4\nindex----------> 5\nTrue  IsNext 0\nindex----------> 6\nindex----------> 7\nindex----------> 8\nindex----------> 9\nindex----------> 10\nTrue  IsNext 1\nindex----------> 11\nindex----------> 12\nTrue  IsNext 2\n[[1, 23, 5, 20, 6, 26, 16, 21, 2, 17, 3, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [6, 21, 0, 0, 0], [10, 11, 0, 0, 0], False]\n[[1, 17, 3, 21, 2, 23, 21, 9, 18, 7, 13, 28, 24, 3, 6, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [10, 6, 0, 0, 0], [13, 2, 0, 0, 0], False]\n[[1, 3, 6, 21, 2, 17, 6, 21, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [17, 0, 0, 0, 0], [1, 0, 0, 0, 0], False]\n[[1, 28, 10, 8, 27, 5, 3, 6, 14, 2, 25, 9, 11, 6, 12, 15, 19, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [22, 6, 20, 0, 0], [13, 3, 6, 0, 0], True]\n[[1, 8, 4, 13, 2, 3, 6, 21, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [17, 0, 0, 0, 0], [5, 0, 0, 0, 0], True]\n[[1, 23, 3, 9, 18, 7, 13, 28, 24, 10, 6, 2, 28, 10, 7, 3, 5, 20, 6, 14, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [27, 21, 6, 0, 0], [15, 2, 14, 0, 0], True]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# BERT Parameters\n",
    "\n",
    "#    vocab_size = len(index2word) = 29      V\n",
    "maxlen = 30          #两个句子cat之后的最大长度\n",
    "batch_size = 6\n",
    "max_pred = 5       # max tokens of prediction      最多的预测长度，取n_pred  = min(   5,len(input_ids)*0.15 )  # 15 % of tokens in one sentence\n",
    "n_layers = 6\n",
    "n_heads = 12\n",
    "d_model = 768     #dimension  D\n",
    "d_ff = 768 * 4  # 4*d_model, FeedForward dimension\n",
    "d_k = d_v = 64  # dimension of K(=Q), V\n",
    "n_segments = 2\n",
    "\n",
    "\n",
    "# sample IsNext and NotNext to be same in small batch size\n",
    "def make_batch():\n",
    "    batch = []\n",
    "    positive = negative = 0       #positive=num_of_isNext    negativa=num_of_NotNext\n",
    "\n",
    "    tmpnum = 0\n",
    "     #每次随机抽取两个句子，一开始大概率是NotNext:False ,直到抽取到一半是IsNext时结束          一共batch_size/2*2=batch_size=6次\n",
    "    while positive != batch_size/2 or negative != batch_size/2:\n",
    "    # if positive != batch_size / 2 or negative != batch_size / 2:\n",
    "        tmpnum += 1\n",
    "        print('index---------->', tmpnum)\n",
    "        # print(len(sentences))        #6\n",
    "        tokens_a_index, tokens_b_index = randrange(len(sentences)), randrange( len(sentences))  # sample random index in sentences\n",
    "        # print(tokens_a_index, tokens_b_index)   # 4,0\n",
    "        #tokens_a,tokens_b 为随机找的两个句子\n",
    "        tokens_a, tokens_b = token_list[tokens_a_index], token_list[tokens_b_index]\n",
    "        #                 [1,       index_of_a,          2,       index_of_b ,                    2]\n",
    "        # <class 'list'>: [1,       26, 10, 15,          2,       5, 13, 25, 20, 22, 6, 11,       2]\n",
    "        input_ids = [word2index['[CLS]']] + tokens_a + [word2index['[SEP]']] + tokens_b + [word2index['[SEP]']]\n",
    "        #<class 'list'>:  [0,        0, 0, 0,            0,        1, 1, 1, 1, 1, 1, 1,            1 ]\n",
    "        segment_ids =    [0] * (1 + len(tokens_a) + 1)        +    [1] * (len(tokens_b) + 1)\n",
    "\n",
    "\n",
    "\n",
    "        # MASK LM                          round(13*0.15)=round(1.94)=2    #len(input_ids) 为两个句子合并之后的长度\n",
    "        n_pred = min(max_pred, max(1, int(round(len(input_ids) * 0.15))))       # 15 % of tokens in one sentence     #2\n",
    "        #<class 'list'>:  [          1, 2, 3,                     5, 6, 7, 8, 9, 10, 11]             去掉1,2之后剩下的token位置\n",
    "        cand_maked_pos = [i for i, token in enumerate(input_ids)\n",
    "                          if token != word2index['[CLS]'] and token != word2index['[SEP]']]\n",
    "        shuffle(cand_maked_pos)        #<class 'list'>: [5, 10, 2, 9, 11, 3, 7, 1, 6, 8]\n",
    "\n",
    "        masked_tokens, masked_pos = [], []\n",
    "        for pos in cand_maked_pos[:n_pred]:       #随机抽取cand_maked_pos中的n_pred=2前两个位置作为mask language model替换   masked_tokens=[5,10]\n",
    "            masked_pos.append(pos)                    # masked_pos=[5,10]\n",
    "            masked_tokens.append(input_ids[pos])      #masked_tokens =[5,6]\n",
    "\n",
    "            #<class 'list'>: [1,       26, 10, 15,          2,       5, 13, 25, 20, 22, 6, 11,       2]\n",
    "            #<class 'list'>: [1,       26, 10, 15,          2,       3, 13, 25, 20, 22, 3, 11,       2]\n",
    "            #         5----->3        6--------->3\n",
    "            if random() < 0.8:  # 80%\n",
    "                input_ids[pos] = word2index['[MASK]']     # make mask        word2index['[MASK]']=3\n",
    "            elif random() < 0.5:  # 10%\n",
    "                index = randint(0, vocab_size - 1)  # random index in vocabulary\n",
    "                input_ids[pos] = word2index[index2word[index]]  # replace\n",
    "\n",
    "        # Zero Paddings\n",
    "        n_pad = maxlen - len(input_ids)        #30-13=17\n",
    "        input_ids.extend([0] * n_pad)         #input_ids     : [1, 26, 10, 15, 2, 3, 13, 25, 20, 22, 3, 11, 2,   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        segment_ids.extend([0] * n_pad)       #segment_ids   : [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "        # Zero Padding (100% - 15%) tokens\n",
    "        if max_pred > n_pred:       #5>2\n",
    "            n_pad = max_pred - n_pred    #3\n",
    "            masked_tokens.extend([0] * n_pad)  #masked_tokens : [5, 6, 0, 0, 0]\n",
    "            masked_pos.extend([0] * n_pad)     #masked_pos    : [5,10, 0, 0, 0]\n",
    "\n",
    "\n",
    "         #每次随机抽取两个句子，一开始大概率是NotNext:False ,直到抽取到一半是IsNext时结束\n",
    "        if tokens_a_index + 1 == tokens_b_index and positive < batch_size / 2:\n",
    "            print(\"True  IsNext\",positive)\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True])  # IsNext\n",
    "            positive += 1\n",
    "        elif tokens_a_index + 1 != tokens_b_index and negative < batch_size / 2:\n",
    "            print(\"False  NotNext\",negative)\n",
    "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False])  # NotNext\n",
    "            negative += 1\n",
    "    \n",
    "    \n",
    "    #input_ids:      token_embedding,   some token has been substituted\n",
    "    #segment_ids:    segment_embedding\n",
    "    # masked_tokens: the token which has been substituded                #作为target，Y\n",
    "    # masked_pos:    the token position which has been substituded\n",
    "    # isNext:        the sentence_a and sentence_b isNext or NotNext       #作为target，Y\n",
    "     \n",
    "    return batch         #batch[i]= [input_ids, segment_ids, masked_tokens, masked_pos, True]\n",
    "\n",
    "\n",
    "# Proprecessing Finished\n",
    "\n",
    "batch = make_batch()\n",
    "\n",
    "for each in batch:\n",
    "    print(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 35.173149\n"
     ]
    }
   ],
   "source": [
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = zip(*batch)\n",
    "print(input_ids,\"\\n\\n\",segment_ids,\"\\n\\n\",masked_tokens,\"\\n\\n\",masked_pos,\"\\n\",isNext)\n",
    "\n",
    "\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = \\\n",
    "    torch.LongTensor(input_ids),  torch.LongTensor(segment_ids), torch.LongTensor(masked_tokens), \\\n",
    "    torch.LongTensor(masked_pos), torch.LongTensor(isNext)\n",
    "\n",
    "\n",
    "print(input_ids.size(),\"\\n\\n\",segment_ids.size(),\"\\n\\n\",masked_tokens.size(),\"\\n\\n\",masked_pos.size(),\"\\n\",isNext.size(),isNext)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost = 35.173149\n"
     ]
    }
   ],
   "source": [
    "def get_attn_pad_mask(seq_q, seq_k):\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    # eq(zero) is PAD token\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), one is masking\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)  #(b,m,m) \n",
    "\n",
    "\n",
    "\n",
    "def gelu(x):\n",
    "    \"Implementation of the gelu activation function by Hugging Face\"\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.tok_embed = nn.Embedding(vocab_size, d_model)  # token embedding\n",
    "        self.pos_embed = nn.Embedding(maxlen, d_model)  # position embedding\n",
    "        self.seg_embed = nn.Embedding(n_segments, d_model)  # segment(token type) embedding\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, seg):        #(input_ids, segment_ids)     (b,m),(b,m)\n",
    "        seq_len = x.size(1)         #max_len=m=30\n",
    "        pos = torch.arange(seq_len, dtype=torch.long)\n",
    "        # print(pos)       tensor([  0,   1,   2,   3,   4,   5...........23, 24,  25,  26,  27,  28,  29])\n",
    "        # (1,m) -------> (b,m)\n",
    "        pos = pos.unsqueeze(0).expand_as(x) \n",
    "        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
    "        # print(embedding)  #(b,m,d) =(6,30,768)\n",
    "        # print(self.norm(embedding))     #(b,m,d) =(6,30,768)\n",
    "        return self.norm(embedding)      #(b,m,d) \n",
    "\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.matmul(attn, V)\n",
    "        return context, attn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n",
    "        residual, batch_size = Q, Q.size(0)\n",
    "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
    "        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]\n",
    "        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]\n",
    "        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]\n",
    "\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size x n_heads x len_q x len_k]\n",
    "\n",
    "        # context: [b,n_heads,m,d_v], attn: [b,n_heads,m,m]\n",
    "        context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v) # context: [b,m, n_heads * d_v]\n",
    "        output = nn.Linear(n_heads * d_v, d_model)(context)\n",
    "        return nn.LayerNorm(d_model)(output + residual), attn      # output: [b,m,d]    attn: [b,n_heads,m,m]\n",
    " \n",
    " \n",
    " \n",
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (b, m, d) -> (b, m, d_ff) -> (b, m, d) \n",
    "        return self.fc2(gelu(self.fc1(x)))\n",
    "\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = PoswiseFeedForwardNet()\n",
    "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
    "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask)   # enc_outputs:(b,m,d)     attn: [b,n_heads,m,m]\n",
    "        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs:(b,m,d) \n",
    "        return enc_outputs, attn      # enc_outputs:(b,m,d)     attn: [b,n_heads,m,m]\n",
    "\n",
    "\n",
    "\n",
    "class BERT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERT, self).__init__()\n",
    "        self.embedding = Embedding()     \n",
    "        \n",
    "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
    "        \n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        self.activ1 = nn.Tanh()\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "        self.activ2 = gelu\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.classifier = nn.Linear(d_model, 2)\n",
    "        # decoder is shared with embedding layer\n",
    "        embed_weight = self.embedding.tok_embed.weight\n",
    "        n_vocab, n_dim = embed_weight.size()\n",
    "        self.decoder = nn.Linear(n_dim, n_vocab, bias=False)\n",
    "        #decoder 和 token_embbedding 的parameter一样\n",
    "        self.decoder.weight = embed_weight\n",
    "        self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))\n",
    "\n",
    "    def forward(self, input_ids, segment_ids, masked_pos):\n",
    "        output = self.embedding(input_ids, segment_ids)       #(b,m,d)\n",
    "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids) #(b,m,m)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            output, enc_self_attn = layer(output, enc_self_attn_mask)       # output:(b,m,d)     attn: [b,n_heads,m,m]\n",
    "            \n",
    "            \n",
    "            \n",
    "        #预测 isNext or NotNext\n",
    "        # output : [b, m, d], attn : [b, n_heads, m,m]\n",
    "        # it will be decided by first token(CLS)\n",
    "        h_pooled = self.activ1(self.fc(output[:, 0])) # [b, d]\n",
    "        logits_clsf = self.classifier(h_pooled) # [b, 2]\n",
    "\n",
    "\n",
    "        #预测  marked_token   (b,m_m,1)------>(b,m_m,d)\n",
    "        masked_pos = masked_pos[:, :, None].expand(-1, -1, output.size(-1)) # [b, m_m, d]   m_m:maxlen_marked\n",
    "        # get masked position from final output of transformer.\n",
    "        h_masked = torch.gather(output, 1, masked_pos) # masking position  [b, m_m, d] \n",
    "        h_masked = self.norm(self.activ2(self.linear(h_masked))) #[b, m_m, d] \n",
    "        logits_lm = self.decoder(h_masked) + self.decoder_bias # [b, m_m, v] \n",
    "        return logits_lm, logits_clsf                          # [b, m_m, v]  , [b, 2]\n",
    "    \n",
    "    \n",
    "model = BERT()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "for epoch in range(1):\n",
    "    optimizer.zero_grad()\n",
    "    logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
    "    loss_lm = criterion(logits_lm.transpose(1, 2), masked_tokens) # for masked LM\n",
    "    loss_lm = (loss_lm.float()).mean()\n",
    "    loss_clsf = criterion(logits_clsf, isNext) # for sentence classification\n",
    "    loss = loss_lm + loss_clsf\n",
    "    if (epoch ) % 10 == 0:\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, how are you? I am Romeo.\nHello, Romeo My name is Juliet. Nice to meet you.\nNice meet you too. How are you today?\nGreat. My baseball team won the competition.\nOh Congratulations, Juliet\nThanks you Romeo\n['[CLS]', 'hello', 'how', 'are', 'you', 'i', 'am', 'romeo', '[SEP]', 'thanks', '[MASK]', '[MASK]', '[SEP]']\nmasked tokens list :  [6, 21]\npredict masked tokens list :  []\nisNext :  False\npredict isNext :  False\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "BERT(\n",
    "  (embedding): Embedding(\n",
    "    (tok_embed): Embedding(29, 768)\n",
    "    (pos_embed): Embedding(30, 768)\n",
    "    (seg_embed): Embedding(2, 768)\n",
    "    (norm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
    "  )\n",
    "  \n",
    "  (layers): ModuleList(\n",
    "    (0-5): EncoderLayer(           #6层encoderlayer\n",
    "      (enc_self_attn): MultiHeadAttention(\n",
    "        (W_Q): Linear(in_features=768, out_features=768, bias=True)\n",
    "        (W_K): Linear(in_features=768, out_features=768, bias=True)\n",
    "        (W_V): Linear(in_features=768, out_features=768, bias=True)\n",
    "      )\n",
    "      (pos_ffn): PoswiseFeedForwardNet(\n",
    "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
    "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
    "      )\n",
    "    )\n",
    "  )\n",
    "  \n",
    "  (fc): Linear(in_features=768, out_features=768, bias=True)\n",
    "  (activ1): Tanh()\n",
    "  (linear): Linear(in_features=768, out_features=768, bias=True)\n",
    "  (norm): LayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)\n",
    "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
    "  (decoder): Linear(in_features=768, out_features=29, bias=False)\n",
    ")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, how are you? I am Romeo.\nHello, Romeo My name is Juliet. Nice to meet you.\nNice meet you too. How are you today?\nGreat. My baseball team won the competition.\nOh Congratulations, Juliet\nThanks you Romeo\n['[CLS]', 'hello', 'how', 'are', 'you', 'i', 'am', 'romeo', '[SEP]', 'thanks', '[MASK]', '[MASK]', '[SEP]']\nmasked tokens list :  [6, 21]\npredict masked tokens list :  []\nisNext :  False\npredict isNext :  False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Predict mask tokens ans isNext\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = batch[0]\n",
    "print(text)\n",
    "print([index2word[w] for w in input_ids if index2word[w] != '[PAD]'])\n",
    "\n",
    "logits_lm, logits_clsf = model(torch.LongTensor([input_ids]), \\\n",
    "                               torch.LongTensor([segment_ids]), torch.LongTensor([masked_pos]))\n",
    "logits_lm = logits_lm.data.max(2)[1][0].data.numpy()\n",
    "print('masked tokens list : ',[pos for pos in masked_tokens if pos != 0])\n",
    "print('predict masked tokens list : ',[pos for pos in logits_lm if pos != 0])\n",
    "\n",
    "logits_clsf = logits_clsf.data.max(1)[1].data.numpy()[0]\n",
    "print('isNext : ', True if isNext else False)\n",
    "print('predict isNext : ',True if logits_clsf else False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(b,m) (b,m)   (b,m_m)  maxlen_maskedb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
