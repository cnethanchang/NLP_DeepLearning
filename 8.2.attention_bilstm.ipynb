{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nv=vocab_size=16\\nb=batch_size=6\\nm=max_len=3\\nd=embedding_dim = 2\\nh= n_hidden = 5 \\no=num_classes = 2         # 0 or 1\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "见下面的jpg  m=3  b=6  v=16    d=2  h=5\n",
    "attention中分为三大快     encode,decode\n",
    "input =  [\"i love you\", \"he loves me\", \"she likes baseball\", \"i hate you\", \"sorry for that\", \"this is awful\"]   (m,b,v)=(3,6,16)\n",
    "output = [1, 1, 1, 0, 0, 0]   (b,o)=(6,2)\n",
    "\n",
    "\n",
    "\n",
    "input: (m,b,v)====>(m,b,d)\n",
    "#input和随机初始化的hidden和cell做LSTM操作，得到output:(m,b,2*h),    final_hidden_state, final_cell_state : =(2,b,h)\n",
    "\n",
    "# final_hidden_state:(b,2*h)和output:(m,b,2*h),的每一层(b,2*h)内积和操作得到m层(1,b),即(m,1,b),然后softmax操作得attention=(b,m,1)====>m方向softmax===(b,m,1)\n",
    "bmm(  (b,m,2*h),(b,2*h,1) ) = (b,m,1)  \n",
    "\n",
    "#output:(b,2*h,m) 和 attention：(b,m,1) 运行bmm（batch  matrix multip)操作得到context= （b,2*h,1）     \n",
    "#                                                      <=============>  等效于每一层output和其attention的乘积，然后相加\n",
    "bmm( (b,2*h,m ),(b,m,1） ） = （b,2*h,1）   \n",
    "\n",
    "\n",
    "\n",
    "#输出\n",
    "(o,2*h) * (2*h,b) = (o,b)\n",
    "(o,b)和ouput运行交叉熵得到loss，反向传播\n",
    "\n",
    "\n",
    "\n",
    "#test操作：\n",
    "input = 'i hate baseball'   #(b,m,v)=(1,3,16)========>(b,m,d)\n",
    "判断输出是0或1\n",
    "\n",
    "predict, _ = model(test_batch)                     # predict : (b,o) =(1,2)\n",
    "# print(predict)                                   # tensor([[ 2.3760, -2.8618]])\n",
    "# print(predict.data.max(1,keepdim=True))          #是元组   (tensor([[ 2.3760]]) , tensor([[ 0]]))\n",
    "predict = predict.data.max(1, keepdim=True)[1]     # tensor([[ 0]]))\n",
    "if predict[0][0] == 0:                             # 表示元组的第二位是0，元组的第一位是最大值，即输出为0===>bad\n",
    "    print(\"'\"+test_text+\"'      \",\"is Bad\")\n",
    "else:\n",
    "    print(\"'\"+test_text+\"'      \",\"is Good \")\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_list= ['he', 'awful', 'she', 'baseball', 'hate', 'sorry', 'i', 'that', 'you', 'likes', 'is', 'this', 'love', 'me', 'loves', 'for']\nword_dict= {'he': 0, 'awful': 1, 'she': 2, 'baseball': 3, 'hate': 4, 'sorry': 5, 'i': 6, 'that': 7, 'you': 8, 'likes': 9, 'is': 10, 'this': 11, 'love': 12, 'me': 13, 'loves': 14, 'for': 15}\nvocab_size =  16\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "\n",
    "# Bi-LSTM(Attention) Parameters\n",
    "embedding_dim = 2\n",
    "n_hidden = 5 # number of hidden units in one cell\n",
    "num_classes = 2  # 0 or 1\n",
    "\n",
    "# 3 words sentences (=sequence_length is 3)\n",
    "sentences = [\"i love you\", \"he loves me\", \"she likes baseball\", \"i hate you\", \"sorry for that\", \"this is awful\"]\n",
    "labels = [1, 1, 1, 0, 0, 0]  # 1 is good, 0 is not good.\n",
    "\n",
    "word_list = \" \".join(sentences).split()\n",
    "word_list = list(set(word_list))\n",
    "word_dict = {w: i for i, w in enumerate(word_list)}\n",
    "vocab_size = len(word_dict)\n",
    "\n",
    "\n",
    "print('word_list=',word_list)\n",
    "print('word_dict=',word_dict)\n",
    "print(\"vocab_size = \" ,vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 6, 12,  8]), array([ 0, 14, 13]), array([2, 9, 3]), array([6, 4, 8]), array([ 5, 15,  7]), array([11, 10,  1])]\ninput= torch.Size([6, 3])\ntarget= torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "inputs = []\n",
    "for sen in sentences:\n",
    "    inputs.append(np.asarray([word_dict[n] for n in sen.split()]))\n",
    "print(inputs)\n",
    "\n",
    "targets = []\n",
    "for out in labels:\n",
    "    targets.append(out)         # To using Torch Softmax Loss function\n",
    "\n",
    "input_batch = Variable(torch.LongTensor(inputs))\n",
    "target_batch = Variable(torch.LongTensor(targets))\n",
    "print(\"input=\",input_batch.size())\n",
    "print(\"target=\",target_batch.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTM_Attention(\n  (embedding): Embedding(16, 2)\n  (lstm): LSTM(2, 5, bidirectional=True)\n  (out): Linear(in_features=10, out_features=2, bias=True)\n)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000 cost = 0.003527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2000 cost = 0.000817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3000 cost = 0.000318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4000 cost = 0.000150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5000 cost = 0.000078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6000 cost = 0.000042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7000 cost = 0.000024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8000 cost = 0.000014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9000 cost = 0.000008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10000 cost = 0.000005\n"
     ]
    }
   ],
   "source": [
    "class BiLSTM_Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BiLSTM_Attention, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)   #(v,d)\n",
    "        self.lstm = nn.LSTM(embedding_dim, n_hidden, bidirectional=True)\n",
    "        self.out = nn.Linear(n_hidden * 2, num_classes)  #(2*h,o)\n",
    "\n",
    "    # lstm_output : [b,m, h * num_directions(=2)] = (b,m,2*h)\n",
    "    def attention_net(self, lstm_output, final_state):\n",
    "        hidden = final_state.view(-1, n_hidden * 2, 1)   # hidden : (2,b,h)====>(b,2*h,1) =(b,hidden * num_directions(=2), n_layer)\n",
    "        attn_weights = torch.bmm(lstm_output, hidden).squeeze(2) #bmm(  (b,m,2*h),(b,2*h,1) ) = (b,m,1)\n",
    "        soft_attn_weights = F.softmax(attn_weights, 1)        #soft_attn_weights =(b,m)  沿着m方向softmax操作\n",
    "        \n",
    "        #bmm( (b,2*h,m ),(b,m,1） ） = （b,2*h,1)  =======>squeeze(2)操作==>(b,2*h)\n",
    "        context = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)    \n",
    "        return context, soft_attn_weights.data.numpy() # context : (b,2*h)\n",
    "\n",
    "    def forward(self, X):  #X:(b,m)\n",
    "        input = self.embedding(X) # input : (b,m,d)\n",
    "        input = input.permute(1, 0, 2) # (b,m,d)====>(m,b,d)\n",
    "\n",
    "        hidden_state = Variable(torch.zeros(1*2, len(X), n_hidden)) # [num_layers(=1) * num_directions(=2), b, h]=(2,b,h)\n",
    "        cell_state = Variable(torch.zeros(1*2, len(X), n_hidden)) # [num_layers(=1) * num_directions(=2), b, h]=(2,b,h)\n",
    "\n",
    "        # output:(m,b,2*h)                     final_hidden_state, final_cell_state : [num_layers(=1) * num_directions(=2), b, h]=(2,b,h)\n",
    "        output, (final_hidden_state, final_cell_state) = self.lstm(input, (hidden_state, cell_state)) # self.lstm(  (m,b,d),  ( (2,b,h),(2,b,h)  )   )\n",
    "        output = output.permute(1, 0, 2) # output :(m,b,2*h) ==>(b,m,2*h)\n",
    "        attn_output, attention = self.attention_net(output, final_hidden_state)        # self.attention_net(  (b,m,2*h),(2,b,h) )\n",
    "        #attn_ouput: (b,2*h)          # model : (2*h,o)*(b,2*h) = (b,o)          attention : (b,m))\n",
    "        return self.out(attn_output), attention \n",
    "\n",
    "\n",
    "model = BiLSTM_Attention()\n",
    "print(model)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training\n",
    "for epoch in range(10000):\n",
    "    optimizer.zero_grad()\n",
    "    #output : (b,o)          attention : (b,m)\n",
    "    output, attention = model(input_batch)    #mode( (b,m) )  = model( (6,3) )\n",
    "\n",
    "    loss = criterion(output, target_batch)   # criterion(  (b,o),  (b) )   \n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'i hate baseball'       is Bad\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test\n",
    "test_text = 'i hate baseball'\n",
    "tests = [np.asarray([word_dict[n] for n in test_text.split()])]\n",
    "test_batch = Variable(torch.LongTensor(tests))\n",
    "\n",
    "predict, _ = model(test_batch)      # predict : (b,o) =(1,2)\n",
    "# print(predict)                     #tensor([[ 2.3760, -2.8618]])\n",
    "# print(predict.data.max(1,keepdim=True))  #是元组   (tensor([[ 2.3760]]) , tensor([[ 0]]))\n",
    "predict = predict.data.max(1, keepdim=True)[1]     #tensor([[ 0]]))\n",
    "if predict[0][0] == 0:            #表示元组的第二位是0，元组的第一位是最大值，即输出为0===>bad\n",
    "    print(\"'\"+test_text+\"'      \",\"is Bad\")\n",
    "else:\n",
    "    print(\"'\"+test_text+\"'      \",\"is Good \")\n",
    "    \n",
    "fig = plt.figure(figsize=(6, 3)) # [batch_size, n_step]\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.matshow(attention, cmap='viridis')\n",
    "ax.set_xticklabels(['']+['first_word', 'second_word', 'third_word'], fontdict={'fontsize': 14}, rotation=90)\n",
    "ax.set_yticklabels(['']+['batch_1', 'batch_2', 'batch_3', 'batch_4', 'batch_5', 'batch_6'], fontdict={'fontsize': 14})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
